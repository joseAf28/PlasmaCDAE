{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import h5py\n",
    "import config as cfg\n",
    "\n",
    "seed = cfg.config_set['seed']\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "###* Prepare data\n",
    "\n",
    "with open(cfg.config_set[\"data_path\"]) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].split()\n",
    "    data[i] = [float(x) for x in data[i]]\n",
    "    \n",
    "data = np.array(data)\n",
    "\n",
    "ratio_test_val_train = cfg.config_set[\"ratio_test_val_train\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data[:, 0:3], data[:, 3:], test_size=ratio_test_val_train, random_state=seed+1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.7, random_state=seed+1)\n",
    "\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_train)\n",
    "y_scaled = scaler_Y.fit_transform(y_train)\n",
    "\n",
    "batch_size = cfg.config_set[\"batch_size\"]\n",
    "\n",
    "dataset = LTPDataset(X_scaled, y_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "y_val_scaled = scaler_Y.transform(y_val)\n",
    "\n",
    "val_dataset = LTPDataset(X_val_scaled, y_val_scaled)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_scaled_test = scaler_X.transform(X_test)\n",
    "y_scaled_test = scaler_Y.transform(y_test)\n",
    "\n",
    "test_dataset = LTPDataset(X_scaled_test, y_scaled_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Define the Conditional Denoising Autoencoder (CDAE) model\n",
    "\n",
    "class ConditionalDenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, ouput_dim=17, cond_dim=3, latent_dim=8, noise_std=0.5):\n",
    "        super(ConditionalDenoisingAutoencoder, self).__init__()\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        ###* Conditional Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(ouput_dim + cond_dim, 200), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, latent_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        ###* Conditional Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 200), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, ouput_dim)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, y, cond, add_noise=True):\n",
    "        if self.training and add_noise:\n",
    "            noise = torch.randn_like(y) * self.noise_std\n",
    "            y_noisy = y + noise\n",
    "        else:\n",
    "            y_noisy = y\n",
    "        \n",
    "        enc_input = torch.cat([y_noisy, cond], dim=1)\n",
    "        latent = self.encoder(enc_input)\n",
    "        \n",
    "        dec_input = torch.cat([latent, cond], dim=1)\n",
    "        y_recon = self.decoder(dec_input)\n",
    "        \n",
    "        return y_recon, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate the model\n",
    "### best noise_std = 0.1\n",
    "\n",
    "latent_dim = 4 \n",
    "cdae = ConditionalDenoisingAutoencoder(ouput_dim=17, cond_dim=3, latent_dim=latent_dim, noise_std=0.05)\n",
    "\n",
    "\n",
    "config_cdae = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"num_epochs\": 400,\n",
    "    \"lambda_sparse\": 8e-3,\n",
    "    \"p_clean\": 0.7,\n",
    "    \"noise_std\": 0.05,\n",
    "\n",
    "    \"latent_dim\": 3,\n",
    "}\n",
    "\n",
    "num_epochs = config_cdae['num_epochs']\n",
    "lr = config_cdae['lr']\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_cdae = optim.Adam(cdae.parameters(), lr=lr)\n",
    "\n",
    "lambda_sparse = config_cdae['lambda_sparse']\n",
    "p_clean = config_cdae['p_clean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 Alternating Autoencoder Loss: 0.29708 Sparse Loss: 0.91099\n",
      "Epoch 2/400 Alternating Autoencoder Loss: 0.04908 Sparse Loss: 0.74067\n",
      "Epoch 3/400 Alternating Autoencoder Loss: 0.02184 Sparse Loss: 1.02337\n",
      "Epoch 4/400 Alternating Autoencoder Loss: 0.01350 Sparse Loss: 0.54008\n",
      "Epoch 5/400 Alternating Autoencoder Loss: 0.01048 Sparse Loss: 0.44178\n",
      "Epoch 6/400 Alternating Autoencoder Loss: 0.00853 Sparse Loss: 0.59197\n",
      "Epoch 7/400 Alternating Autoencoder Loss: 0.00721 Sparse Loss: 0.68102\n",
      "Epoch 8/400 Alternating Autoencoder Loss: 0.00655 Sparse Loss: 0.54587\n",
      "Epoch 9/400 Alternating Autoencoder Loss: 0.00575 Sparse Loss: 0.43812\n",
      "Epoch 10/400 Alternating Autoencoder Loss: 0.00547 Sparse Loss: 0.59769\n",
      "Epoch 11/400 Alternating Autoencoder Loss: 0.00481 Sparse Loss: 0.21312\n",
      "Epoch 12/400 Alternating Autoencoder Loss: 0.00466 Sparse Loss: 0.25402\n",
      "Epoch 13/400 Alternating Autoencoder Loss: 0.00445 Sparse Loss: 0.26482\n",
      "Epoch 14/400 Alternating Autoencoder Loss: 0.00406 Sparse Loss: 0.22943\n",
      "Epoch 15/400 Alternating Autoencoder Loss: 0.00428 Sparse Loss: 0.26380\n",
      "Epoch 16/400 Alternating Autoencoder Loss: 0.00385 Sparse Loss: 0.46732\n",
      "Epoch 17/400 Alternating Autoencoder Loss: 0.00336 Sparse Loss: 0.32191\n",
      "Epoch 18/400 Alternating Autoencoder Loss: 0.00331 Sparse Loss: 0.38079\n",
      "Epoch 19/400 Alternating Autoencoder Loss: 0.00320 Sparse Loss: 0.15345\n",
      "Epoch 20/400 Alternating Autoencoder Loss: 0.00298 Sparse Loss: 0.24895\n",
      "Epoch 21/400 Alternating Autoencoder Loss: 0.00389 Sparse Loss: 0.73471\n",
      "Epoch 22/400 Alternating Autoencoder Loss: 0.00355 Sparse Loss: 0.13347\n",
      "Epoch 23/400 Alternating Autoencoder Loss: 0.00540 Sparse Loss: 0.21791\n",
      "Epoch 24/400 Alternating Autoencoder Loss: 0.00291 Sparse Loss: 0.13232\n",
      "Epoch 25/400 Alternating Autoencoder Loss: 0.00261 Sparse Loss: 0.11566\n",
      "Epoch 26/400 Alternating Autoencoder Loss: 0.00243 Sparse Loss: 0.16106\n",
      "Epoch 27/400 Alternating Autoencoder Loss: 0.00258 Sparse Loss: 0.16395\n",
      "Epoch 28/400 Alternating Autoencoder Loss: 0.00248 Sparse Loss: 0.15978\n",
      "Epoch 29/400 Alternating Autoencoder Loss: 0.00227 Sparse Loss: 0.19766\n",
      "Epoch 30/400 Alternating Autoencoder Loss: 0.00229 Sparse Loss: 0.13564\n",
      "Epoch 31/400 Alternating Autoencoder Loss: 0.00219 Sparse Loss: 0.25802\n",
      "Epoch 32/400 Alternating Autoencoder Loss: 0.00225 Sparse Loss: 0.13617\n",
      "Epoch 33/400 Alternating Autoencoder Loss: 0.00212 Sparse Loss: 0.23836\n",
      "Epoch 34/400 Alternating Autoencoder Loss: 0.00267 Sparse Loss: 0.12940\n",
      "Epoch 35/400 Alternating Autoencoder Loss: 0.00210 Sparse Loss: 0.12987\n",
      "Epoch 36/400 Alternating Autoencoder Loss: 0.00223 Sparse Loss: 0.14077\n",
      "Epoch 37/400 Alternating Autoencoder Loss: 0.00451 Sparse Loss: 0.08233\n",
      "Epoch 38/400 Alternating Autoencoder Loss: 0.00217 Sparse Loss: 0.15038\n",
      "Epoch 39/400 Alternating Autoencoder Loss: 0.00188 Sparse Loss: 0.17708\n",
      "Epoch 40/400 Alternating Autoencoder Loss: 0.00190 Sparse Loss: 0.26264\n",
      "Epoch 41/400 Alternating Autoencoder Loss: 0.00188 Sparse Loss: 0.09124\n",
      "Epoch 42/400 Alternating Autoencoder Loss: 0.00176 Sparse Loss: 0.18493\n",
      "Epoch 43/400 Alternating Autoencoder Loss: 0.00167 Sparse Loss: 0.08023\n",
      "Epoch 44/400 Alternating Autoencoder Loss: 0.00174 Sparse Loss: 0.10689\n",
      "Epoch 45/400 Alternating Autoencoder Loss: 0.00190 Sparse Loss: 0.06205\n",
      "Epoch 46/400 Alternating Autoencoder Loss: 0.00160 Sparse Loss: 0.37860\n",
      "Epoch 47/400 Alternating Autoencoder Loss: 0.00177 Sparse Loss: 0.13920\n",
      "Epoch 48/400 Alternating Autoencoder Loss: 0.00274 Sparse Loss: 0.13870\n",
      "Epoch 49/400 Alternating Autoencoder Loss: 0.00205 Sparse Loss: 0.10564\n",
      "Epoch 50/400 Alternating Autoencoder Loss: 0.00142 Sparse Loss: 0.08566\n",
      "Epoch 51/400 Alternating Autoencoder Loss: 0.00161 Sparse Loss: 0.09263\n",
      "Epoch 52/400 Alternating Autoencoder Loss: 0.00157 Sparse Loss: 0.10785\n",
      "Epoch 53/400 Alternating Autoencoder Loss: 0.00139 Sparse Loss: 0.10363\n",
      "Epoch 54/400 Alternating Autoencoder Loss: 0.00192 Sparse Loss: 0.12269\n",
      "Epoch 55/400 Alternating Autoencoder Loss: 0.00161 Sparse Loss: 0.07952\n",
      "Epoch 56/400 Alternating Autoencoder Loss: 0.00180 Sparse Loss: 0.35527\n",
      "Epoch 57/400 Alternating Autoencoder Loss: 0.00147 Sparse Loss: 0.08427\n",
      "Epoch 58/400 Alternating Autoencoder Loss: 0.00142 Sparse Loss: 0.08063\n",
      "Epoch 59/400 Alternating Autoencoder Loss: 0.00159 Sparse Loss: 0.10009\n",
      "Epoch 60/400 Alternating Autoencoder Loss: 0.00137 Sparse Loss: 0.05453\n",
      "Epoch 61/400 Alternating Autoencoder Loss: 0.00121 Sparse Loss: 0.08320\n",
      "Epoch 62/400 Alternating Autoencoder Loss: 0.00160 Sparse Loss: 0.04060\n",
      "Epoch 63/400 Alternating Autoencoder Loss: 0.00153 Sparse Loss: 0.11334\n",
      "Epoch 64/400 Alternating Autoencoder Loss: 0.00169 Sparse Loss: 0.07093\n",
      "Epoch 65/400 Alternating Autoencoder Loss: 0.00152 Sparse Loss: 0.05700\n",
      "Epoch 66/400 Alternating Autoencoder Loss: 0.00144 Sparse Loss: 0.05678\n",
      "Epoch 67/400 Alternating Autoencoder Loss: 0.00113 Sparse Loss: 0.06732\n",
      "Epoch 68/400 Alternating Autoencoder Loss: 0.00134 Sparse Loss: 0.06031\n",
      "Epoch 69/400 Alternating Autoencoder Loss: 0.00118 Sparse Loss: 0.08644\n",
      "Epoch 70/400 Alternating Autoencoder Loss: 0.00266 Sparse Loss: 0.15008\n",
      "Epoch 71/400 Alternating Autoencoder Loss: 0.00145 Sparse Loss: 0.15942\n",
      "Epoch 72/400 Alternating Autoencoder Loss: 0.00110 Sparse Loss: 0.07597\n",
      "Epoch 73/400 Alternating Autoencoder Loss: 0.00256 Sparse Loss: 0.06782\n",
      "Epoch 74/400 Alternating Autoencoder Loss: 0.00249 Sparse Loss: 0.07720\n",
      "Epoch 75/400 Alternating Autoencoder Loss: 0.00112 Sparse Loss: 0.02654\n",
      "Epoch 76/400 Alternating Autoencoder Loss: 0.00117 Sparse Loss: 0.18445\n",
      "Epoch 77/400 Alternating Autoencoder Loss: 0.00147 Sparse Loss: 0.08600\n",
      "Epoch 78/400 Alternating Autoencoder Loss: 0.00099 Sparse Loss: 0.07887\n",
      "Epoch 79/400 Alternating Autoencoder Loss: 0.00116 Sparse Loss: 0.02923\n",
      "Epoch 80/400 Alternating Autoencoder Loss: 0.00119 Sparse Loss: 0.05272\n",
      "Epoch 81/400 Alternating Autoencoder Loss: 0.00110 Sparse Loss: 0.04004\n",
      "Epoch 82/400 Alternating Autoencoder Loss: 0.00091 Sparse Loss: 0.04084\n",
      "Epoch 83/400 Alternating Autoencoder Loss: 0.00115 Sparse Loss: 0.17063\n",
      "Epoch 84/400 Alternating Autoencoder Loss: 0.00171 Sparse Loss: 0.07095\n",
      "Epoch 85/400 Alternating Autoencoder Loss: 0.00122 Sparse Loss: 0.04375\n",
      "Epoch 86/400 Alternating Autoencoder Loss: 0.00118 Sparse Loss: 0.03423\n",
      "Epoch 87/400 Alternating Autoencoder Loss: 0.00093 Sparse Loss: 0.04070\n",
      "Epoch 88/400 Alternating Autoencoder Loss: 0.00106 Sparse Loss: 0.17302\n",
      "Epoch 89/400 Alternating Autoencoder Loss: 0.00098 Sparse Loss: 0.05509\n",
      "Epoch 90/400 Alternating Autoencoder Loss: 0.00085 Sparse Loss: 0.03840\n",
      "Epoch 91/400 Alternating Autoencoder Loss: 0.00132 Sparse Loss: 0.13392\n",
      "Epoch 92/400 Alternating Autoencoder Loss: 0.00090 Sparse Loss: 0.05789\n",
      "Epoch 93/400 Alternating Autoencoder Loss: 0.00082 Sparse Loss: 0.05021\n",
      "Epoch 94/400 Alternating Autoencoder Loss: 0.00092 Sparse Loss: 0.03170\n",
      "Epoch 95/400 Alternating Autoencoder Loss: 0.00119 Sparse Loss: 0.06541\n",
      "Epoch 96/400 Alternating Autoencoder Loss: 0.00256 Sparse Loss: 0.07724\n",
      "Epoch 97/400 Alternating Autoencoder Loss: 0.00130 Sparse Loss: 0.06273\n",
      "Epoch 98/400 Alternating Autoencoder Loss: 0.00113 Sparse Loss: 0.07658\n",
      "Epoch 99/400 Alternating Autoencoder Loss: 0.00100 Sparse Loss: 0.04913\n",
      "Epoch 100/400 Alternating Autoencoder Loss: 0.00131 Sparse Loss: 0.06881\n",
      "Epoch 101/400 Alternating Autoencoder Loss: 0.00094 Sparse Loss: 0.02120\n",
      "Epoch 102/400 Alternating Autoencoder Loss: 0.00099 Sparse Loss: 0.06494\n",
      "Epoch 103/400 Alternating Autoencoder Loss: 0.00142 Sparse Loss: 0.02957\n",
      "Epoch 104/400 Alternating Autoencoder Loss: 0.00114 Sparse Loss: 0.09368\n",
      "Epoch 105/400 Alternating Autoencoder Loss: 0.00152 Sparse Loss: 0.06331\n",
      "Epoch 106/400 Alternating Autoencoder Loss: 0.00091 Sparse Loss: 0.06519\n",
      "Epoch 107/400 Alternating Autoencoder Loss: 0.00123 Sparse Loss: 0.08177\n",
      "Epoch 108/400 Alternating Autoencoder Loss: 0.00104 Sparse Loss: 0.01965\n",
      "Epoch 109/400 Alternating Autoencoder Loss: 0.00089 Sparse Loss: 0.07818\n",
      "Epoch 110/400 Alternating Autoencoder Loss: 0.00083 Sparse Loss: 0.03591\n",
      "Epoch 111/400 Alternating Autoencoder Loss: 0.00100 Sparse Loss: 0.04673\n",
      "Epoch 112/400 Alternating Autoencoder Loss: 0.00087 Sparse Loss: 0.07081\n",
      "Epoch 113/400 Alternating Autoencoder Loss: 0.00114 Sparse Loss: 0.05922\n",
      "Epoch 114/400 Alternating Autoencoder Loss: 0.00102 Sparse Loss: 0.05284\n",
      "Epoch 115/400 Alternating Autoencoder Loss: 0.00082 Sparse Loss: 0.02202\n",
      "Epoch 116/400 Alternating Autoencoder Loss: 0.00085 Sparse Loss: 0.03344\n",
      "Epoch 117/400 Alternating Autoencoder Loss: 0.00076 Sparse Loss: 0.03688\n",
      "Epoch 118/400 Alternating Autoencoder Loss: 0.00074 Sparse Loss: 0.05043\n",
      "Epoch 119/400 Alternating Autoencoder Loss: 0.00113 Sparse Loss: 0.01063\n",
      "Epoch 120/400 Alternating Autoencoder Loss: 0.00111 Sparse Loss: 0.04451\n",
      "Epoch 121/400 Alternating Autoencoder Loss: 0.00074 Sparse Loss: 0.02343\n",
      "Epoch 122/400 Alternating Autoencoder Loss: 0.00069 Sparse Loss: 0.05454\n",
      "Epoch 123/400 Alternating Autoencoder Loss: 0.00066 Sparse Loss: 0.04783\n",
      "Epoch 124/400 Alternating Autoencoder Loss: 0.00088 Sparse Loss: 0.04022\n",
      "Epoch 125/400 Alternating Autoencoder Loss: 0.00121 Sparse Loss: 0.03828\n",
      "Epoch 126/400 Alternating Autoencoder Loss: 0.00098 Sparse Loss: 0.05807\n",
      "Epoch 127/400 Alternating Autoencoder Loss: 0.00085 Sparse Loss: 0.02435\n",
      "Epoch 128/400 Alternating Autoencoder Loss: 0.00089 Sparse Loss: 0.05455\n",
      "Epoch 129/400 Alternating Autoencoder Loss: 0.00082 Sparse Loss: 0.04878\n",
      "Epoch 130/400 Alternating Autoencoder Loss: 0.00066 Sparse Loss: 0.04162\n",
      "Epoch 131/400 Alternating Autoencoder Loss: 0.00133 Sparse Loss: 0.03623\n",
      "Epoch 132/400 Alternating Autoencoder Loss: 0.00215 Sparse Loss: 0.12173\n",
      "Epoch 133/400 Alternating Autoencoder Loss: 0.00093 Sparse Loss: 0.02364\n",
      "Epoch 134/400 Alternating Autoencoder Loss: 0.00066 Sparse Loss: 0.06335\n",
      "Epoch 135/400 Alternating Autoencoder Loss: 0.00123 Sparse Loss: 0.03278\n",
      "Epoch 136/400 Alternating Autoencoder Loss: 0.00142 Sparse Loss: 0.09177\n",
      "Epoch 137/400 Alternating Autoencoder Loss: 0.00070 Sparse Loss: 0.01566\n",
      "Epoch 138/400 Alternating Autoencoder Loss: 0.00114 Sparse Loss: 0.07071\n",
      "Epoch 139/400 Alternating Autoencoder Loss: 0.00072 Sparse Loss: 0.06278\n",
      "Epoch 140/400 Alternating Autoencoder Loss: 0.00061 Sparse Loss: 0.03936\n",
      "Epoch 141/400 Alternating Autoencoder Loss: 0.00075 Sparse Loss: 0.03830\n",
      "Epoch 142/400 Alternating Autoencoder Loss: 0.00067 Sparse Loss: 0.00966\n",
      "Epoch 143/400 Alternating Autoencoder Loss: 0.00059 Sparse Loss: 0.02775\n",
      "Epoch 144/400 Alternating Autoencoder Loss: 0.00091 Sparse Loss: 0.03964\n",
      "Epoch 145/400 Alternating Autoencoder Loss: 0.00068 Sparse Loss: 0.03898\n",
      "Epoch 146/400 Alternating Autoencoder Loss: 0.00067 Sparse Loss: 0.04059\n",
      "Epoch 147/400 Alternating Autoencoder Loss: 0.00058 Sparse Loss: 0.01140\n",
      "Epoch 148/400 Alternating Autoencoder Loss: 0.00065 Sparse Loss: 0.04501\n",
      "Epoch 149/400 Alternating Autoencoder Loss: 0.00079 Sparse Loss: 0.01891\n",
      "Epoch 150/400 Alternating Autoencoder Loss: 0.00117 Sparse Loss: 0.04437\n",
      "Epoch 151/400 Alternating Autoencoder Loss: 0.00520 Sparse Loss: 0.04142\n",
      "Epoch 152/400 Alternating Autoencoder Loss: 0.00139 Sparse Loss: 0.01359\n",
      "Epoch 153/400 Alternating Autoencoder Loss: 0.00063 Sparse Loss: 0.01649\n",
      "Epoch 154/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.02882\n",
      "Epoch 155/400 Alternating Autoencoder Loss: 0.00053 Sparse Loss: 0.02449\n",
      "Epoch 156/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.02401\n",
      "Epoch 157/400 Alternating Autoencoder Loss: 0.00059 Sparse Loss: 0.04089\n",
      "Epoch 158/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.01459\n",
      "Epoch 159/400 Alternating Autoencoder Loss: 0.00052 Sparse Loss: 0.06054\n",
      "Epoch 160/400 Alternating Autoencoder Loss: 0.00056 Sparse Loss: 0.00749\n",
      "Epoch 161/400 Alternating Autoencoder Loss: 0.00073 Sparse Loss: 0.06263\n",
      "Epoch 162/400 Alternating Autoencoder Loss: 0.00058 Sparse Loss: 0.00950\n",
      "Epoch 163/400 Alternating Autoencoder Loss: 0.00048 Sparse Loss: 0.01533\n",
      "Epoch 164/400 Alternating Autoencoder Loss: 0.00053 Sparse Loss: 0.01600\n",
      "Epoch 165/400 Alternating Autoencoder Loss: 0.00052 Sparse Loss: 0.03900\n",
      "Epoch 166/400 Alternating Autoencoder Loss: 0.00060 Sparse Loss: 0.07628\n",
      "Epoch 167/400 Alternating Autoencoder Loss: 0.00062 Sparse Loss: 0.03097\n",
      "Epoch 168/400 Alternating Autoencoder Loss: 0.00079 Sparse Loss: 0.02528\n",
      "Epoch 169/400 Alternating Autoencoder Loss: 0.00276 Sparse Loss: 0.02800\n",
      "Epoch 170/400 Alternating Autoencoder Loss: 0.00220 Sparse Loss: 0.02903\n",
      "Epoch 171/400 Alternating Autoencoder Loss: 0.00057 Sparse Loss: 0.05347\n",
      "Epoch 172/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.03059\n",
      "Epoch 173/400 Alternating Autoencoder Loss: 0.00052 Sparse Loss: 0.04130\n",
      "Epoch 174/400 Alternating Autoencoder Loss: 0.00074 Sparse Loss: 0.04525\n",
      "Epoch 175/400 Alternating Autoencoder Loss: 0.00061 Sparse Loss: 0.04488\n",
      "Epoch 176/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.02299\n",
      "Epoch 177/400 Alternating Autoencoder Loss: 0.00052 Sparse Loss: 0.03729\n",
      "Epoch 178/400 Alternating Autoencoder Loss: 0.00053 Sparse Loss: 0.01013\n",
      "Epoch 179/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.01204\n",
      "Epoch 180/400 Alternating Autoencoder Loss: 0.00068 Sparse Loss: 0.03562\n",
      "Epoch 181/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.12248\n",
      "Epoch 182/400 Alternating Autoencoder Loss: 0.00069 Sparse Loss: 0.01772\n",
      "Epoch 183/400 Alternating Autoencoder Loss: 0.00141 Sparse Loss: 0.04394\n",
      "Epoch 184/400 Alternating Autoencoder Loss: 0.00063 Sparse Loss: 0.05432\n",
      "Epoch 185/400 Alternating Autoencoder Loss: 0.00056 Sparse Loss: 0.03392\n",
      "Epoch 186/400 Alternating Autoencoder Loss: 0.00095 Sparse Loss: 0.03310\n",
      "Epoch 187/400 Alternating Autoencoder Loss: 0.00056 Sparse Loss: 0.03394\n",
      "Epoch 188/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00631\n",
      "Epoch 189/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.03435\n",
      "Epoch 190/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.01474\n",
      "Epoch 191/400 Alternating Autoencoder Loss: 0.00048 Sparse Loss: 0.03573\n",
      "Epoch 192/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.00816\n",
      "Epoch 193/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.02305\n",
      "Epoch 194/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.04230\n",
      "Epoch 195/400 Alternating Autoencoder Loss: 0.00077 Sparse Loss: 0.06577\n",
      "Epoch 196/400 Alternating Autoencoder Loss: 0.00052 Sparse Loss: 0.01836\n",
      "Epoch 197/400 Alternating Autoencoder Loss: 0.00063 Sparse Loss: 0.05037\n",
      "Epoch 198/400 Alternating Autoencoder Loss: 0.00319 Sparse Loss: 0.01209\n",
      "Epoch 199/400 Alternating Autoencoder Loss: 0.00160 Sparse Loss: 0.03046\n",
      "Epoch 200/400 Alternating Autoencoder Loss: 0.00088 Sparse Loss: 0.01118\n",
      "Epoch 201/400 Alternating Autoencoder Loss: 0.00060 Sparse Loss: 0.07307\n",
      "Epoch 202/400 Alternating Autoencoder Loss: 0.00045 Sparse Loss: 0.08778\n",
      "Epoch 203/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.00643\n",
      "Epoch 204/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.03416\n",
      "Epoch 205/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.07632\n",
      "Epoch 206/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.01962\n",
      "Epoch 207/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.00926\n",
      "Epoch 208/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.01397\n",
      "Epoch 209/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.01482\n",
      "Epoch 210/400 Alternating Autoencoder Loss: 0.00074 Sparse Loss: 0.06829\n",
      "Epoch 211/400 Alternating Autoencoder Loss: 0.00047 Sparse Loss: 0.04122\n",
      "Epoch 212/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.00400\n",
      "Epoch 213/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.01489\n",
      "Epoch 214/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.01712\n",
      "Epoch 215/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.05572\n",
      "Epoch 216/400 Alternating Autoencoder Loss: 0.00061 Sparse Loss: 0.03563\n",
      "Epoch 217/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.03855\n",
      "Epoch 218/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00261\n",
      "Epoch 219/400 Alternating Autoencoder Loss: 0.00156 Sparse Loss: 0.04156\n",
      "Epoch 220/400 Alternating Autoencoder Loss: 0.00070 Sparse Loss: 0.02639\n",
      "Epoch 221/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.00775\n",
      "Epoch 222/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.04133\n",
      "Epoch 223/400 Alternating Autoencoder Loss: 0.00075 Sparse Loss: 0.01553\n",
      "Epoch 224/400 Alternating Autoencoder Loss: 0.00114 Sparse Loss: 0.01357\n",
      "Epoch 225/400 Alternating Autoencoder Loss: 0.00188 Sparse Loss: 0.02097\n",
      "Epoch 226/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.01873\n",
      "Epoch 227/400 Alternating Autoencoder Loss: 0.00047 Sparse Loss: 0.02177\n",
      "Epoch 228/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.01962\n",
      "Epoch 229/400 Alternating Autoencoder Loss: 0.00083 Sparse Loss: 0.01662\n",
      "Epoch 230/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.01897\n",
      "Epoch 231/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.03610\n",
      "Epoch 232/400 Alternating Autoencoder Loss: 0.00058 Sparse Loss: 0.02133\n",
      "Epoch 233/400 Alternating Autoencoder Loss: 0.00049 Sparse Loss: 0.03799\n",
      "Epoch 234/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00861\n",
      "Epoch 235/400 Alternating Autoencoder Loss: 0.00066 Sparse Loss: 0.02616\n",
      "Epoch 236/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.02832\n",
      "Epoch 237/400 Alternating Autoencoder Loss: 0.00045 Sparse Loss: 0.02515\n",
      "Epoch 238/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.02241\n",
      "Epoch 239/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.00849\n",
      "Epoch 240/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.03442\n",
      "Epoch 241/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.01842\n",
      "Epoch 242/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.01967\n",
      "Epoch 243/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.02209\n",
      "Epoch 244/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.02768\n",
      "Epoch 245/400 Alternating Autoencoder Loss: 0.00048 Sparse Loss: 0.01133\n",
      "Epoch 246/400 Alternating Autoencoder Loss: 0.00048 Sparse Loss: 0.01445\n",
      "Epoch 247/400 Alternating Autoencoder Loss: 0.00136 Sparse Loss: 0.01080\n",
      "Epoch 248/400 Alternating Autoencoder Loss: 0.00073 Sparse Loss: 0.04149\n",
      "Epoch 249/400 Alternating Autoencoder Loss: 0.00059 Sparse Loss: 0.02820\n",
      "Epoch 250/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.08269\n",
      "Epoch 251/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.06126\n",
      "Epoch 252/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.00587\n",
      "Epoch 253/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.00939\n",
      "Epoch 254/400 Alternating Autoencoder Loss: 0.00062 Sparse Loss: 0.02250\n",
      "Epoch 255/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.01342\n",
      "Epoch 256/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.01998\n",
      "Epoch 257/400 Alternating Autoencoder Loss: 0.00045 Sparse Loss: 0.02848\n",
      "Epoch 258/400 Alternating Autoencoder Loss: 0.00127 Sparse Loss: 0.00563\n",
      "Epoch 259/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.00765\n",
      "Epoch 260/400 Alternating Autoencoder Loss: 0.00101 Sparse Loss: 0.02041\n",
      "Epoch 261/400 Alternating Autoencoder Loss: 0.00139 Sparse Loss: 0.00462\n",
      "Epoch 262/400 Alternating Autoencoder Loss: 0.00060 Sparse Loss: 0.01733\n",
      "Epoch 263/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00633\n",
      "Epoch 264/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.03293\n",
      "Epoch 265/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01286\n",
      "Epoch 266/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01503\n",
      "Epoch 267/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01469\n",
      "Epoch 268/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.00581\n",
      "Epoch 269/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.00638\n",
      "Epoch 270/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.02933\n",
      "Epoch 271/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.00896\n",
      "Epoch 272/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.00932\n",
      "Epoch 273/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.01953\n",
      "Epoch 274/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.00502\n",
      "Epoch 275/400 Alternating Autoencoder Loss: 0.00055 Sparse Loss: 0.01254\n",
      "Epoch 276/400 Alternating Autoencoder Loss: 0.00100 Sparse Loss: 0.00997\n",
      "Epoch 277/400 Alternating Autoencoder Loss: 0.00196 Sparse Loss: 0.05059\n",
      "Epoch 278/400 Alternating Autoencoder Loss: 0.00070 Sparse Loss: 0.01273\n",
      "Epoch 279/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.01243\n",
      "Epoch 280/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.02201\n",
      "Epoch 281/400 Alternating Autoencoder Loss: 0.00084 Sparse Loss: 0.01046\n",
      "Epoch 282/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.01696\n",
      "Epoch 283/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.00676\n",
      "Epoch 284/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.01566\n",
      "Epoch 285/400 Alternating Autoencoder Loss: 0.00047 Sparse Loss: 0.01022\n",
      "Epoch 286/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.02074\n",
      "Epoch 287/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.02597\n",
      "Epoch 288/400 Alternating Autoencoder Loss: 0.00054 Sparse Loss: 0.01907\n",
      "Epoch 289/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01202\n",
      "Epoch 290/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.00858\n",
      "Epoch 291/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.02503\n",
      "Epoch 292/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.02557\n",
      "Epoch 293/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.01593\n",
      "Epoch 294/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.04131\n",
      "Epoch 295/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.03099\n",
      "Epoch 296/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.00851\n",
      "Epoch 297/400 Alternating Autoencoder Loss: 0.00063 Sparse Loss: 0.00953\n",
      "Epoch 298/400 Alternating Autoencoder Loss: 0.00057 Sparse Loss: 0.01183\n",
      "Epoch 299/400 Alternating Autoencoder Loss: 0.00051 Sparse Loss: 0.01887\n",
      "Epoch 300/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.01403\n",
      "Epoch 301/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.01245\n",
      "Epoch 302/400 Alternating Autoencoder Loss: 0.00072 Sparse Loss: 0.02349\n",
      "Epoch 303/400 Alternating Autoencoder Loss: 0.00062 Sparse Loss: 0.03168\n",
      "Epoch 304/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.01497\n",
      "Epoch 305/400 Alternating Autoencoder Loss: 0.00049 Sparse Loss: 0.01566\n",
      "Epoch 306/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.00366\n",
      "Epoch 307/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00535\n",
      "Epoch 308/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.03451\n",
      "Epoch 309/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.02196\n",
      "Epoch 310/400 Alternating Autoencoder Loss: 0.00057 Sparse Loss: 0.01095\n",
      "Epoch 311/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.04847\n",
      "Epoch 312/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00865\n",
      "Epoch 313/400 Alternating Autoencoder Loss: 0.00056 Sparse Loss: 0.01228\n",
      "Epoch 314/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.00580\n",
      "Epoch 315/400 Alternating Autoencoder Loss: 0.00042 Sparse Loss: 0.01019\n",
      "Epoch 316/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00472\n",
      "Epoch 317/400 Alternating Autoencoder Loss: 0.00030 Sparse Loss: 0.02523\n",
      "Epoch 318/400 Alternating Autoencoder Loss: 0.00045 Sparse Loss: 0.01980\n",
      "Epoch 319/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.03104\n",
      "Epoch 320/400 Alternating Autoencoder Loss: 0.00062 Sparse Loss: 0.00801\n",
      "Epoch 321/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.00841\n",
      "Epoch 322/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.00261\n",
      "Epoch 323/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.01386\n",
      "Epoch 324/400 Alternating Autoencoder Loss: 0.00048 Sparse Loss: 0.02227\n",
      "Epoch 325/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00394\n",
      "Epoch 326/400 Alternating Autoencoder Loss: 0.00030 Sparse Loss: 0.00408\n",
      "Epoch 327/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.00911\n",
      "Epoch 328/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.01116\n",
      "Epoch 329/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.02359\n",
      "Epoch 330/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.00871\n",
      "Epoch 331/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.01191\n",
      "Epoch 332/400 Alternating Autoencoder Loss: 0.00082 Sparse Loss: 0.01088\n",
      "Epoch 333/400 Alternating Autoencoder Loss: 0.00071 Sparse Loss: 0.00558\n",
      "Epoch 334/400 Alternating Autoencoder Loss: 0.00050 Sparse Loss: 0.09712\n",
      "Epoch 335/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.01713\n",
      "Epoch 336/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.00123\n",
      "Epoch 337/400 Alternating Autoencoder Loss: 0.00029 Sparse Loss: 0.05663\n",
      "Epoch 338/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.01509\n",
      "Epoch 339/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.02935\n",
      "Epoch 340/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.00335\n",
      "Epoch 341/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.02197\n",
      "Epoch 342/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.03349\n",
      "Epoch 343/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01574\n",
      "Epoch 344/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.00627\n",
      "Epoch 345/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.04976\n",
      "Epoch 346/400 Alternating Autoencoder Loss: 0.00076 Sparse Loss: 0.01558\n",
      "Epoch 347/400 Alternating Autoencoder Loss: 0.00082 Sparse Loss: 0.01485\n",
      "Epoch 348/400 Alternating Autoencoder Loss: 0.00063 Sparse Loss: 0.01516\n",
      "Epoch 349/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.01404\n",
      "Epoch 350/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.03054\n",
      "Epoch 351/400 Alternating Autoencoder Loss: 0.00031 Sparse Loss: 0.00328\n",
      "Epoch 352/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00488\n",
      "Epoch 353/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.02172\n",
      "Epoch 354/400 Alternating Autoencoder Loss: 0.00053 Sparse Loss: 0.03051\n",
      "Epoch 355/400 Alternating Autoencoder Loss: 0.00077 Sparse Loss: 0.00268\n",
      "Epoch 356/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.04249\n",
      "Epoch 357/400 Alternating Autoencoder Loss: 0.00028 Sparse Loss: 0.01090\n",
      "Epoch 358/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.00582\n",
      "Epoch 359/400 Alternating Autoencoder Loss: 0.00031 Sparse Loss: 0.00693\n",
      "Epoch 360/400 Alternating Autoencoder Loss: 0.00038 Sparse Loss: 0.00525\n",
      "Epoch 361/400 Alternating Autoencoder Loss: 0.00056 Sparse Loss: 0.02699\n",
      "Epoch 362/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.01419\n",
      "Epoch 363/400 Alternating Autoencoder Loss: 0.00049 Sparse Loss: 0.02025\n",
      "Epoch 364/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.02509\n",
      "Epoch 365/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.00230\n",
      "Epoch 366/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.01542\n",
      "Epoch 367/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.00165\n",
      "Epoch 368/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.00201\n",
      "Epoch 369/400 Alternating Autoencoder Loss: 0.00030 Sparse Loss: 0.01376\n",
      "Epoch 370/400 Alternating Autoencoder Loss: 0.00039 Sparse Loss: 0.01231\n",
      "Epoch 371/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00355\n",
      "Epoch 372/400 Alternating Autoencoder Loss: 0.00045 Sparse Loss: 0.04578\n",
      "Epoch 373/400 Alternating Autoencoder Loss: 0.00047 Sparse Loss: 0.03128\n",
      "Epoch 374/400 Alternating Autoencoder Loss: 0.00061 Sparse Loss: 0.00861\n",
      "Epoch 375/400 Alternating Autoencoder Loss: 0.00034 Sparse Loss: 0.05006\n",
      "Epoch 376/400 Alternating Autoencoder Loss: 0.00041 Sparse Loss: 0.01766\n",
      "Epoch 377/400 Alternating Autoencoder Loss: 0.00053 Sparse Loss: 0.00694\n",
      "Epoch 378/400 Alternating Autoencoder Loss: 0.00037 Sparse Loss: 0.01338\n",
      "Epoch 379/400 Alternating Autoencoder Loss: 0.00032 Sparse Loss: 0.00886\n",
      "Epoch 380/400 Alternating Autoencoder Loss: 0.00031 Sparse Loss: 0.00303\n",
      "Epoch 381/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00805\n",
      "Epoch 382/400 Alternating Autoencoder Loss: 0.00059 Sparse Loss: 0.00246\n",
      "Epoch 383/400 Alternating Autoencoder Loss: 0.00047 Sparse Loss: 0.01159\n",
      "Epoch 384/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.00659\n",
      "Epoch 385/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.02976\n",
      "Epoch 386/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.04349\n",
      "Epoch 387/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.00907\n",
      "Epoch 388/400 Alternating Autoencoder Loss: 0.00044 Sparse Loss: 0.00311\n",
      "Epoch 389/400 Alternating Autoencoder Loss: 0.00040 Sparse Loss: 0.01005\n",
      "Epoch 390/400 Alternating Autoencoder Loss: 0.00046 Sparse Loss: 0.02100\n",
      "Epoch 391/400 Alternating Autoencoder Loss: 0.00029 Sparse Loss: 0.01611\n",
      "Epoch 392/400 Alternating Autoencoder Loss: 0.00035 Sparse Loss: 0.01583\n",
      "Epoch 393/400 Alternating Autoencoder Loss: 0.00033 Sparse Loss: 0.00353\n",
      "Epoch 394/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.01074\n",
      "Epoch 395/400 Alternating Autoencoder Loss: 0.00026 Sparse Loss: 0.00399\n",
      "Epoch 396/400 Alternating Autoencoder Loss: 0.00027 Sparse Loss: 0.00372\n",
      "Epoch 397/400 Alternating Autoencoder Loss: 0.00036 Sparse Loss: 0.01525\n",
      "Epoch 398/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.00737\n",
      "Epoch 399/400 Alternating Autoencoder Loss: 0.00061 Sparse Loss: 0.01917\n",
      "Epoch 400/400 Alternating Autoencoder Loss: 0.00043 Sparse Loss: 0.01478\n"
     ]
    }
   ],
   "source": [
    "#### Train the conditional denoising autoencoder\n",
    "\n",
    "cdae.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for cond, y in dataloader:\n",
    "        optimizer_cdae.zero_grad()\n",
    "        \n",
    "        if np.random.rand() < p_clean:\n",
    "            add_noise = False\n",
    "        else:\n",
    "            add_noise = True\n",
    "        \n",
    "        \n",
    "        y_recon, latent = cdae(y, cond, add_noise=add_noise)\n",
    "        \n",
    "        \n",
    "        loss_sparse = torch.mean(torch.abs(latent))\n",
    "        \n",
    "        loss = criterion(y_recon, y) + lambda_sparse * loss_sparse\n",
    "        loss.backward()\n",
    "        optimizer_cdae.step()\n",
    "        epoch_loss += loss.item() * cond.size(0)\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Alternating Autoencoder Loss: {epoch_loss:.5f} Sparse Loss: {loss_sparse.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.00019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model_cdae(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    y_recon_vec = []\n",
    "    with torch.no_grad():\n",
    "        for cond, y in dataloader:\n",
    "            y_recon, _ = model(y, cond, add_noise=False)\n",
    "            loss = criterion(y_recon, y)\n",
    "            epoch_loss += loss.item() * cond.size(0)\n",
    "        \n",
    "            y_recon_vec.append(y_recon.detach().numpy())\n",
    "            \n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    y_recon_vec = np.concatenate(y_recon_vec, axis=0)\n",
    "    \n",
    "    return epoch_loss, y_recon_vec\n",
    "\n",
    "\n",
    "loss_cdae, y_recon = evaluate_model_cdae(cdae, test_dataloader, criterion)\n",
    "print(f\"Test Loss: {loss_cdae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, cond_dim=3, latent_dim=8):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "        \n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        return self.residual(cond)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 Mapping Network Loss: 0.0529444 Loss Theta: 32.67643\n",
      "Epoch 2/300 Mapping Network Loss: 0.0366312 Loss Theta: 26.85452\n",
      "Epoch 3/300 Mapping Network Loss: 0.0304944 Loss Theta: 21.58237\n",
      "Epoch 4/300 Mapping Network Loss: 0.0253232 Loss Theta: 17.15053\n",
      "Epoch 5/300 Mapping Network Loss: 0.0211441 Loss Theta: 13.53085\n",
      "Epoch 6/300 Mapping Network Loss: 0.0177486 Loss Theta: 10.68656\n",
      "Epoch 7/300 Mapping Network Loss: 0.0151356 Loss Theta: 8.47647\n",
      "Epoch 8/300 Mapping Network Loss: 0.0130688 Loss Theta: 6.78231\n",
      "Epoch 9/300 Mapping Network Loss: 0.0114903 Loss Theta: 5.48857\n",
      "Epoch 10/300 Mapping Network Loss: 0.0103073 Loss Theta: 4.48259\n",
      "Epoch 11/300 Mapping Network Loss: 0.0094034 Loss Theta: 3.75442\n",
      "Epoch 12/300 Mapping Network Loss: 0.0087015 Loss Theta: 3.19369\n",
      "Epoch 13/300 Mapping Network Loss: 0.0081854 Loss Theta: 2.77425\n",
      "Epoch 14/300 Mapping Network Loss: 0.0077571 Loss Theta: 2.42163\n",
      "Epoch 15/300 Mapping Network Loss: 0.0074614 Loss Theta: 2.23135\n",
      "Epoch 16/300 Mapping Network Loss: 0.0072064 Loss Theta: 2.00667\n",
      "Epoch 17/300 Mapping Network Loss: 0.0070498 Loss Theta: 1.89299\n",
      "Epoch 18/300 Mapping Network Loss: 0.0069048 Loss Theta: 1.72792\n",
      "Epoch 19/300 Mapping Network Loss: 0.0067525 Loss Theta: 1.68194\n",
      "Epoch 20/300 Mapping Network Loss: 0.0066566 Loss Theta: 1.64274\n",
      "Epoch 21/300 Mapping Network Loss: 0.0066162 Loss Theta: 1.59417\n",
      "Epoch 22/300 Mapping Network Loss: 0.0065587 Loss Theta: 1.53116\n",
      "Epoch 23/300 Mapping Network Loss: 0.0064949 Loss Theta: 1.51253\n",
      "Epoch 24/300 Mapping Network Loss: 0.0063926 Loss Theta: 1.49360\n",
      "Epoch 25/300 Mapping Network Loss: 0.0063570 Loss Theta: 1.46021\n",
      "Epoch 26/300 Mapping Network Loss: 0.0063588 Loss Theta: 1.44445\n",
      "Epoch 27/300 Mapping Network Loss: 0.0063903 Loss Theta: 1.44067\n",
      "Epoch 28/300 Mapping Network Loss: 0.0062780 Loss Theta: 1.46833\n",
      "Epoch 29/300 Mapping Network Loss: 0.0062926 Loss Theta: 1.44992\n",
      "Epoch 30/300 Mapping Network Loss: 0.0062691 Loss Theta: 1.41855\n",
      "Epoch 31/300 Mapping Network Loss: 0.0062893 Loss Theta: 1.43856\n",
      "Epoch 32/300 Mapping Network Loss: 0.0062264 Loss Theta: 1.41585\n",
      "Epoch 33/300 Mapping Network Loss: 0.0062275 Loss Theta: 1.43587\n",
      "Epoch 34/300 Mapping Network Loss: 0.0061986 Loss Theta: 1.44272\n",
      "Epoch 35/300 Mapping Network Loss: 0.0062209 Loss Theta: 1.42351\n",
      "Epoch 36/300 Mapping Network Loss: 0.0061931 Loss Theta: 1.43409\n",
      "Epoch 37/300 Mapping Network Loss: 0.0062162 Loss Theta: 1.43275\n",
      "Epoch 38/300 Mapping Network Loss: 0.0062457 Loss Theta: 1.44283\n",
      "Epoch 39/300 Mapping Network Loss: 0.0062226 Loss Theta: 1.45976\n",
      "Epoch 40/300 Mapping Network Loss: 0.0061538 Loss Theta: 1.47449\n",
      "Epoch 41/300 Mapping Network Loss: 0.0061698 Loss Theta: 1.44127\n",
      "Epoch 42/300 Mapping Network Loss: 0.0061649 Loss Theta: 1.45604\n",
      "Epoch 43/300 Mapping Network Loss: 0.0061788 Loss Theta: 1.48382\n",
      "Epoch 44/300 Mapping Network Loss: 0.0061470 Loss Theta: 1.47500\n",
      "Epoch 45/300 Mapping Network Loss: 0.0061455 Loss Theta: 1.50134\n",
      "Epoch 46/300 Mapping Network Loss: 0.0061283 Loss Theta: 1.48781\n",
      "Epoch 47/300 Mapping Network Loss: 0.0061428 Loss Theta: 1.47550\n",
      "Epoch 48/300 Mapping Network Loss: 0.0061699 Loss Theta: 1.49238\n",
      "Epoch 49/300 Mapping Network Loss: 0.0061851 Loss Theta: 1.50370\n",
      "Epoch 50/300 Mapping Network Loss: 0.0061736 Loss Theta: 1.52206\n",
      "Epoch 51/300 Mapping Network Loss: 0.0061410 Loss Theta: 1.52762\n",
      "Epoch 52/300 Mapping Network Loss: 0.0061559 Loss Theta: 1.51314\n",
      "Epoch 53/300 Mapping Network Loss: 0.0061404 Loss Theta: 1.53255\n",
      "Epoch 54/300 Mapping Network Loss: 0.0061243 Loss Theta: 1.54066\n",
      "Epoch 55/300 Mapping Network Loss: 0.0061465 Loss Theta: 1.46863\n",
      "Epoch 56/300 Mapping Network Loss: 0.0062617 Loss Theta: 1.52787\n",
      "Epoch 57/300 Mapping Network Loss: 0.0061241 Loss Theta: 1.53412\n",
      "Epoch 58/300 Mapping Network Loss: 0.0061448 Loss Theta: 1.52119\n",
      "Epoch 59/300 Mapping Network Loss: 0.0062291 Loss Theta: 1.54959\n",
      "Epoch 60/300 Mapping Network Loss: 0.0061275 Loss Theta: 1.52086\n",
      "Epoch 61/300 Mapping Network Loss: 0.0061249 Loss Theta: 1.50283\n",
      "Epoch 62/300 Mapping Network Loss: 0.0061125 Loss Theta: 1.53978\n",
      "Epoch 63/300 Mapping Network Loss: 0.0061190 Loss Theta: 1.57056\n",
      "Epoch 64/300 Mapping Network Loss: 0.0061289 Loss Theta: 1.51475\n",
      "Epoch 65/300 Mapping Network Loss: 0.0061459 Loss Theta: 1.53927\n",
      "Epoch 66/300 Mapping Network Loss: 0.0060977 Loss Theta: 1.53675\n",
      "Epoch 67/300 Mapping Network Loss: 0.0061216 Loss Theta: 1.53732\n",
      "Epoch 68/300 Mapping Network Loss: 0.0061018 Loss Theta: 1.52486\n",
      "Epoch 69/300 Mapping Network Loss: 0.0061941 Loss Theta: 1.55294\n",
      "Epoch 70/300 Mapping Network Loss: 0.0061225 Loss Theta: 1.54486\n",
      "Epoch 71/300 Mapping Network Loss: 0.0061137 Loss Theta: 1.53512\n",
      "Epoch 72/300 Mapping Network Loss: 0.0061017 Loss Theta: 1.56738\n",
      "Epoch 73/300 Mapping Network Loss: 0.0061091 Loss Theta: 1.54884\n",
      "Epoch 74/300 Mapping Network Loss: 0.0061336 Loss Theta: 1.58467\n",
      "Epoch 75/300 Mapping Network Loss: 0.0061672 Loss Theta: 1.60036\n",
      "Epoch 76/300 Mapping Network Loss: 0.0061647 Loss Theta: 1.60877\n",
      "Epoch 77/300 Mapping Network Loss: 0.0061207 Loss Theta: 1.59509\n",
      "Epoch 78/300 Mapping Network Loss: 0.0061201 Loss Theta: 1.52632\n",
      "Epoch 79/300 Mapping Network Loss: 0.0061090 Loss Theta: 1.52369\n",
      "Epoch 80/300 Mapping Network Loss: 0.0061549 Loss Theta: 1.55493\n",
      "Epoch 81/300 Mapping Network Loss: 0.0060958 Loss Theta: 1.53831\n",
      "Epoch 82/300 Mapping Network Loss: 0.0061570 Loss Theta: 1.53932\n",
      "Epoch 83/300 Mapping Network Loss: 0.0061142 Loss Theta: 1.54930\n",
      "Epoch 84/300 Mapping Network Loss: 0.0061471 Loss Theta: 1.54109\n",
      "Epoch 85/300 Mapping Network Loss: 0.0061225 Loss Theta: 1.53931\n",
      "Epoch 86/300 Mapping Network Loss: 0.0060931 Loss Theta: 1.56616\n",
      "Epoch 87/300 Mapping Network Loss: 0.0060899 Loss Theta: 1.60569\n",
      "Epoch 88/300 Mapping Network Loss: 0.0060995 Loss Theta: 1.59166\n",
      "Epoch 89/300 Mapping Network Loss: 0.0061030 Loss Theta: 1.61174\n",
      "Epoch 90/300 Mapping Network Loss: 0.0061094 Loss Theta: 1.55988\n",
      "Epoch 91/300 Mapping Network Loss: 0.0060978 Loss Theta: 1.59066\n",
      "Epoch 92/300 Mapping Network Loss: 0.0061052 Loss Theta: 1.62658\n",
      "Epoch 93/300 Mapping Network Loss: 0.0061116 Loss Theta: 1.55471\n",
      "Epoch 94/300 Mapping Network Loss: 0.0061043 Loss Theta: 1.59661\n",
      "Epoch 95/300 Mapping Network Loss: 0.0060624 Loss Theta: 1.56592\n",
      "Epoch 96/300 Mapping Network Loss: 0.0060928 Loss Theta: 1.56470\n",
      "Epoch 97/300 Mapping Network Loss: 0.0061224 Loss Theta: 1.54398\n",
      "Epoch 98/300 Mapping Network Loss: 0.0060907 Loss Theta: 1.58670\n",
      "Epoch 99/300 Mapping Network Loss: 0.0061243 Loss Theta: 1.55791\n",
      "Epoch 100/300 Mapping Network Loss: 0.0061019 Loss Theta: 1.56919\n",
      "Epoch 101/300 Mapping Network Loss: 0.0061312 Loss Theta: 1.56831\n",
      "Epoch 102/300 Mapping Network Loss: 0.0060937 Loss Theta: 1.59377\n",
      "Epoch 103/300 Mapping Network Loss: 0.0060655 Loss Theta: 1.60990\n",
      "Epoch 104/300 Mapping Network Loss: 0.0061113 Loss Theta: 1.59343\n",
      "Epoch 105/300 Mapping Network Loss: 0.0060766 Loss Theta: 1.62959\n",
      "Epoch 106/300 Mapping Network Loss: 0.0060705 Loss Theta: 1.58059\n",
      "Epoch 107/300 Mapping Network Loss: 0.0060826 Loss Theta: 1.60326\n",
      "Epoch 108/300 Mapping Network Loss: 0.0060804 Loss Theta: 1.58429\n",
      "Epoch 109/300 Mapping Network Loss: 0.0060804 Loss Theta: 1.60131\n",
      "Epoch 110/300 Mapping Network Loss: 0.0061098 Loss Theta: 1.60912\n",
      "Epoch 111/300 Mapping Network Loss: 0.0060790 Loss Theta: 1.60253\n",
      "Epoch 112/300 Mapping Network Loss: 0.0060808 Loss Theta: 1.62684\n",
      "Epoch 113/300 Mapping Network Loss: 0.0060714 Loss Theta: 1.57612\n",
      "Epoch 114/300 Mapping Network Loss: 0.0061510 Loss Theta: 1.58754\n",
      "Epoch 115/300 Mapping Network Loss: 0.0060939 Loss Theta: 1.60177\n",
      "Epoch 116/300 Mapping Network Loss: 0.0060627 Loss Theta: 1.63392\n",
      "Epoch 117/300 Mapping Network Loss: 0.0061232 Loss Theta: 1.63522\n",
      "Epoch 118/300 Mapping Network Loss: 0.0060807 Loss Theta: 1.58848\n",
      "Epoch 119/300 Mapping Network Loss: 0.0060925 Loss Theta: 1.58838\n",
      "Epoch 120/300 Mapping Network Loss: 0.0060640 Loss Theta: 1.65695\n",
      "Epoch 121/300 Mapping Network Loss: 0.0060721 Loss Theta: 1.60727\n",
      "Epoch 122/300 Mapping Network Loss: 0.0060662 Loss Theta: 1.64195\n",
      "Epoch 123/300 Mapping Network Loss: 0.0061011 Loss Theta: 1.56919\n",
      "Epoch 124/300 Mapping Network Loss: 0.0061113 Loss Theta: 1.60630\n",
      "Epoch 125/300 Mapping Network Loss: 0.0060997 Loss Theta: 1.63959\n",
      "Epoch 126/300 Mapping Network Loss: 0.0061011 Loss Theta: 1.59647\n",
      "Epoch 127/300 Mapping Network Loss: 0.0060734 Loss Theta: 1.62398\n",
      "Epoch 128/300 Mapping Network Loss: 0.0060724 Loss Theta: 1.60621\n",
      "Epoch 129/300 Mapping Network Loss: 0.0060724 Loss Theta: 1.62799\n",
      "Epoch 130/300 Mapping Network Loss: 0.0060826 Loss Theta: 1.59386\n",
      "Epoch 131/300 Mapping Network Loss: 0.0060664 Loss Theta: 1.64367\n",
      "Epoch 132/300 Mapping Network Loss: 0.0060764 Loss Theta: 1.60901\n",
      "Epoch 133/300 Mapping Network Loss: 0.0060947 Loss Theta: 1.60577\n",
      "Epoch 134/300 Mapping Network Loss: 0.0060623 Loss Theta: 1.60370\n",
      "Epoch 135/300 Mapping Network Loss: 0.0060664 Loss Theta: 1.61382\n",
      "Epoch 136/300 Mapping Network Loss: 0.0060923 Loss Theta: 1.60069\n",
      "Epoch 137/300 Mapping Network Loss: 0.0060903 Loss Theta: 1.64661\n",
      "Epoch 138/300 Mapping Network Loss: 0.0061067 Loss Theta: 1.63961\n",
      "Epoch 139/300 Mapping Network Loss: 0.0060787 Loss Theta: 1.65364\n",
      "Epoch 140/300 Mapping Network Loss: 0.0060945 Loss Theta: 1.66448\n",
      "Epoch 141/300 Mapping Network Loss: 0.0060763 Loss Theta: 1.62488\n",
      "Epoch 142/300 Mapping Network Loss: 0.0060669 Loss Theta: 1.63451\n",
      "Epoch 143/300 Mapping Network Loss: 0.0060775 Loss Theta: 1.64106\n",
      "Epoch 144/300 Mapping Network Loss: 0.0060666 Loss Theta: 1.64758\n",
      "Epoch 145/300 Mapping Network Loss: 0.0060586 Loss Theta: 1.62226\n",
      "Epoch 146/300 Mapping Network Loss: 0.0060748 Loss Theta: 1.64022\n",
      "Epoch 147/300 Mapping Network Loss: 0.0060739 Loss Theta: 1.62143\n",
      "Epoch 148/300 Mapping Network Loss: 0.0060852 Loss Theta: 1.62839\n",
      "Epoch 149/300 Mapping Network Loss: 0.0060622 Loss Theta: 1.63327\n",
      "Epoch 150/300 Mapping Network Loss: 0.0060676 Loss Theta: 1.66899\n",
      "Epoch 151/300 Mapping Network Loss: 0.0060712 Loss Theta: 1.63630\n",
      "Epoch 152/300 Mapping Network Loss: 0.0060712 Loss Theta: 1.62864\n",
      "Epoch 153/300 Mapping Network Loss: 0.0060634 Loss Theta: 1.64148\n",
      "Epoch 154/300 Mapping Network Loss: 0.0060779 Loss Theta: 1.67035\n",
      "Epoch 155/300 Mapping Network Loss: 0.0060547 Loss Theta: 1.61842\n",
      "Epoch 156/300 Mapping Network Loss: 0.0060588 Loss Theta: 1.62971\n",
      "Epoch 157/300 Mapping Network Loss: 0.0060911 Loss Theta: 1.60680\n",
      "Epoch 158/300 Mapping Network Loss: 0.0061459 Loss Theta: 1.59055\n",
      "Epoch 159/300 Mapping Network Loss: 0.0060828 Loss Theta: 1.63636\n",
      "Epoch 160/300 Mapping Network Loss: 0.0060648 Loss Theta: 1.60239\n",
      "Epoch 161/300 Mapping Network Loss: 0.0060789 Loss Theta: 1.60145\n",
      "Epoch 162/300 Mapping Network Loss: 0.0060804 Loss Theta: 1.64908\n",
      "Epoch 163/300 Mapping Network Loss: 0.0060832 Loss Theta: 1.66190\n",
      "Epoch 164/300 Mapping Network Loss: 0.0060690 Loss Theta: 1.65508\n",
      "Epoch 165/300 Mapping Network Loss: 0.0060814 Loss Theta: 1.60237\n",
      "Epoch 166/300 Mapping Network Loss: 0.0061560 Loss Theta: 1.63434\n",
      "Epoch 167/300 Mapping Network Loss: 0.0060861 Loss Theta: 1.65172\n",
      "Epoch 168/300 Mapping Network Loss: 0.0060575 Loss Theta: 1.61776\n",
      "Epoch 169/300 Mapping Network Loss: 0.0060477 Loss Theta: 1.63386\n",
      "Epoch 170/300 Mapping Network Loss: 0.0060692 Loss Theta: 1.66458\n",
      "Epoch 171/300 Mapping Network Loss: 0.0060881 Loss Theta: 1.63202\n",
      "Epoch 172/300 Mapping Network Loss: 0.0060860 Loss Theta: 1.64783\n",
      "Epoch 173/300 Mapping Network Loss: 0.0060780 Loss Theta: 1.64473\n",
      "Epoch 174/300 Mapping Network Loss: 0.0060727 Loss Theta: 1.62080\n",
      "Epoch 175/300 Mapping Network Loss: 0.0061015 Loss Theta: 1.62637\n",
      "Epoch 176/300 Mapping Network Loss: 0.0060688 Loss Theta: 1.62612\n",
      "Epoch 177/300 Mapping Network Loss: 0.0060666 Loss Theta: 1.66330\n",
      "Epoch 178/300 Mapping Network Loss: 0.0060523 Loss Theta: 1.61844\n",
      "Epoch 179/300 Mapping Network Loss: 0.0061138 Loss Theta: 1.59969\n",
      "Epoch 180/300 Mapping Network Loss: 0.0060662 Loss Theta: 1.63040\n",
      "Epoch 181/300 Mapping Network Loss: 0.0060811 Loss Theta: 1.67399\n",
      "Epoch 182/300 Mapping Network Loss: 0.0060672 Loss Theta: 1.61815\n",
      "Epoch 183/300 Mapping Network Loss: 0.0060721 Loss Theta: 1.64887\n",
      "Epoch 184/300 Mapping Network Loss: 0.0060524 Loss Theta: 1.59955\n",
      "Epoch 185/300 Mapping Network Loss: 0.0060982 Loss Theta: 1.58567\n",
      "Epoch 186/300 Mapping Network Loss: 0.0060643 Loss Theta: 1.63932\n",
      "Epoch 187/300 Mapping Network Loss: 0.0060754 Loss Theta: 1.63492\n",
      "Epoch 188/300 Mapping Network Loss: 0.0060637 Loss Theta: 1.61682\n",
      "Epoch 189/300 Mapping Network Loss: 0.0060864 Loss Theta: 1.62387\n",
      "Epoch 190/300 Mapping Network Loss: 0.0060618 Loss Theta: 1.60074\n",
      "Epoch 191/300 Mapping Network Loss: 0.0060699 Loss Theta: 1.64625\n",
      "Epoch 192/300 Mapping Network Loss: 0.0060471 Loss Theta: 1.58541\n",
      "Epoch 193/300 Mapping Network Loss: 0.0060984 Loss Theta: 1.64583\n",
      "Epoch 194/300 Mapping Network Loss: 0.0060588 Loss Theta: 1.60773\n",
      "Epoch 195/300 Mapping Network Loss: 0.0060958 Loss Theta: 1.62349\n",
      "Epoch 196/300 Mapping Network Loss: 0.0060573 Loss Theta: 1.65395\n",
      "Epoch 197/300 Mapping Network Loss: 0.0061244 Loss Theta: 1.62807\n",
      "Epoch 198/300 Mapping Network Loss: 0.0061084 Loss Theta: 1.64753\n",
      "Epoch 199/300 Mapping Network Loss: 0.0060905 Loss Theta: 1.61936\n",
      "Epoch 200/300 Mapping Network Loss: 0.0060563 Loss Theta: 1.64144\n",
      "Epoch 201/300 Mapping Network Loss: 0.0060662 Loss Theta: 1.64543\n",
      "Epoch 202/300 Mapping Network Loss: 0.0061164 Loss Theta: 1.61787\n",
      "Epoch 203/300 Mapping Network Loss: 0.0060427 Loss Theta: 1.67834\n",
      "Epoch 204/300 Mapping Network Loss: 0.0061066 Loss Theta: 1.64866\n",
      "Epoch 205/300 Mapping Network Loss: 0.0060724 Loss Theta: 1.60036\n",
      "Epoch 206/300 Mapping Network Loss: 0.0060592 Loss Theta: 1.64878\n",
      "Epoch 207/300 Mapping Network Loss: 0.0060674 Loss Theta: 1.61711\n",
      "Epoch 208/300 Mapping Network Loss: 0.0061202 Loss Theta: 1.64820\n",
      "Epoch 209/300 Mapping Network Loss: 0.0061063 Loss Theta: 1.62680\n",
      "Epoch 210/300 Mapping Network Loss: 0.0061166 Loss Theta: 1.66645\n",
      "Epoch 211/300 Mapping Network Loss: 0.0060707 Loss Theta: 1.67483\n",
      "Epoch 212/300 Mapping Network Loss: 0.0060635 Loss Theta: 1.62238\n",
      "Epoch 213/300 Mapping Network Loss: 0.0060980 Loss Theta: 1.62819\n",
      "Epoch 214/300 Mapping Network Loss: 0.0060535 Loss Theta: 1.63222\n",
      "Epoch 215/300 Mapping Network Loss: 0.0060538 Loss Theta: 1.65548\n",
      "Epoch 216/300 Mapping Network Loss: 0.0060756 Loss Theta: 1.62627\n",
      "Epoch 217/300 Mapping Network Loss: 0.0060506 Loss Theta: 1.63365\n",
      "Epoch 218/300 Mapping Network Loss: 0.0060636 Loss Theta: 1.61491\n",
      "Epoch 219/300 Mapping Network Loss: 0.0060839 Loss Theta: 1.65537\n",
      "Epoch 220/300 Mapping Network Loss: 0.0060763 Loss Theta: 1.68083\n",
      "Epoch 221/300 Mapping Network Loss: 0.0060463 Loss Theta: 1.61119\n",
      "Epoch 222/300 Mapping Network Loss: 0.0060504 Loss Theta: 1.68217\n",
      "Epoch 223/300 Mapping Network Loss: 0.0061229 Loss Theta: 1.61666\n",
      "Epoch 224/300 Mapping Network Loss: 0.0060639 Loss Theta: 1.67862\n",
      "Epoch 225/300 Mapping Network Loss: 0.0060785 Loss Theta: 1.66661\n",
      "Epoch 226/300 Mapping Network Loss: 0.0061658 Loss Theta: 1.67621\n",
      "Epoch 227/300 Mapping Network Loss: 0.0060914 Loss Theta: 1.64199\n",
      "Epoch 228/300 Mapping Network Loss: 0.0060618 Loss Theta: 1.65054\n",
      "Epoch 229/300 Mapping Network Loss: 0.0060827 Loss Theta: 1.62064\n",
      "Epoch 230/300 Mapping Network Loss: 0.0061559 Loss Theta: 1.64870\n",
      "Epoch 231/300 Mapping Network Loss: 0.0060773 Loss Theta: 1.62277\n",
      "Epoch 232/300 Mapping Network Loss: 0.0060690 Loss Theta: 1.63280\n",
      "Epoch 233/300 Mapping Network Loss: 0.0060729 Loss Theta: 1.62943\n",
      "Epoch 234/300 Mapping Network Loss: 0.0061045 Loss Theta: 1.62495\n",
      "Epoch 235/300 Mapping Network Loss: 0.0061292 Loss Theta: 1.59568\n",
      "Epoch 236/300 Mapping Network Loss: 0.0060690 Loss Theta: 1.65835\n",
      "Epoch 237/300 Mapping Network Loss: 0.0060933 Loss Theta: 1.59365\n",
      "Epoch 238/300 Mapping Network Loss: 0.0060583 Loss Theta: 1.64900\n",
      "Epoch 239/300 Mapping Network Loss: 0.0060512 Loss Theta: 1.62279\n",
      "Epoch 240/300 Mapping Network Loss: 0.0060690 Loss Theta: 1.66686\n",
      "Epoch 241/300 Mapping Network Loss: 0.0060649 Loss Theta: 1.63243\n",
      "Epoch 242/300 Mapping Network Loss: 0.0060775 Loss Theta: 1.66452\n",
      "Epoch 243/300 Mapping Network Loss: 0.0060935 Loss Theta: 1.67182\n",
      "Epoch 244/300 Mapping Network Loss: 0.0060456 Loss Theta: 1.64009\n",
      "Epoch 245/300 Mapping Network Loss: 0.0060871 Loss Theta: 1.62516\n",
      "Epoch 246/300 Mapping Network Loss: 0.0060521 Loss Theta: 1.68985\n",
      "Epoch 247/300 Mapping Network Loss: 0.0060953 Loss Theta: 1.66083\n",
      "Epoch 248/300 Mapping Network Loss: 0.0061199 Loss Theta: 1.67397\n",
      "Epoch 249/300 Mapping Network Loss: 0.0060938 Loss Theta: 1.64025\n",
      "Epoch 250/300 Mapping Network Loss: 0.0060432 Loss Theta: 1.66077\n",
      "Epoch 251/300 Mapping Network Loss: 0.0060789 Loss Theta: 1.67764\n",
      "Epoch 252/300 Mapping Network Loss: 0.0060793 Loss Theta: 1.63709\n",
      "Epoch 253/300 Mapping Network Loss: 0.0060535 Loss Theta: 1.63745\n",
      "Epoch 254/300 Mapping Network Loss: 0.0060578 Loss Theta: 1.65074\n",
      "Epoch 255/300 Mapping Network Loss: 0.0060517 Loss Theta: 1.65453\n",
      "Epoch 256/300 Mapping Network Loss: 0.0060703 Loss Theta: 1.67193\n",
      "Epoch 257/300 Mapping Network Loss: 0.0060701 Loss Theta: 1.62920\n",
      "Epoch 258/300 Mapping Network Loss: 0.0060597 Loss Theta: 1.62843\n",
      "Epoch 259/300 Mapping Network Loss: 0.0060639 Loss Theta: 1.68316\n",
      "Epoch 260/300 Mapping Network Loss: 0.0060979 Loss Theta: 1.68523\n",
      "Epoch 261/300 Mapping Network Loss: 0.0061086 Loss Theta: 1.68938\n",
      "Epoch 262/300 Mapping Network Loss: 0.0060716 Loss Theta: 1.64070\n",
      "Epoch 263/300 Mapping Network Loss: 0.0061085 Loss Theta: 1.67651\n",
      "Epoch 264/300 Mapping Network Loss: 0.0060419 Loss Theta: 1.62618\n",
      "Epoch 265/300 Mapping Network Loss: 0.0061037 Loss Theta: 1.65778\n",
      "Epoch 266/300 Mapping Network Loss: 0.0060684 Loss Theta: 1.65692\n",
      "Epoch 267/300 Mapping Network Loss: 0.0060721 Loss Theta: 1.61747\n",
      "Epoch 268/300 Mapping Network Loss: 0.0060604 Loss Theta: 1.66114\n",
      "Epoch 269/300 Mapping Network Loss: 0.0060922 Loss Theta: 1.64163\n",
      "Epoch 270/300 Mapping Network Loss: 0.0060633 Loss Theta: 1.61665\n",
      "Epoch 271/300 Mapping Network Loss: 0.0060749 Loss Theta: 1.64814\n",
      "Epoch 272/300 Mapping Network Loss: 0.0060747 Loss Theta: 1.68186\n",
      "Epoch 273/300 Mapping Network Loss: 0.0060454 Loss Theta: 1.62064\n",
      "Epoch 274/300 Mapping Network Loss: 0.0060737 Loss Theta: 1.63579\n",
      "Epoch 275/300 Mapping Network Loss: 0.0060709 Loss Theta: 1.63939\n",
      "Epoch 276/300 Mapping Network Loss: 0.0060796 Loss Theta: 1.63067\n",
      "Epoch 277/300 Mapping Network Loss: 0.0060794 Loss Theta: 1.63610\n",
      "Epoch 278/300 Mapping Network Loss: 0.0060989 Loss Theta: 1.62863\n",
      "Epoch 279/300 Mapping Network Loss: 0.0060903 Loss Theta: 1.63211\n",
      "Epoch 280/300 Mapping Network Loss: 0.0060583 Loss Theta: 1.64682\n",
      "Epoch 281/300 Mapping Network Loss: 0.0060918 Loss Theta: 1.67747\n",
      "Epoch 282/300 Mapping Network Loss: 0.0060588 Loss Theta: 1.62671\n",
      "Epoch 283/300 Mapping Network Loss: 0.0060661 Loss Theta: 1.62696\n",
      "Epoch 284/300 Mapping Network Loss: 0.0060544 Loss Theta: 1.66660\n",
      "Epoch 285/300 Mapping Network Loss: 0.0060417 Loss Theta: 1.61423\n",
      "Epoch 286/300 Mapping Network Loss: 0.0060794 Loss Theta: 1.64354\n",
      "Epoch 287/300 Mapping Network Loss: 0.0060592 Loss Theta: 1.66039\n",
      "Epoch 288/300 Mapping Network Loss: 0.0060541 Loss Theta: 1.65770\n",
      "Epoch 289/300 Mapping Network Loss: 0.0060473 Loss Theta: 1.61595\n",
      "Epoch 290/300 Mapping Network Loss: 0.0060712 Loss Theta: 1.64411\n",
      "Epoch 291/300 Mapping Network Loss: 0.0060579 Loss Theta: 1.62674\n",
      "Epoch 292/300 Mapping Network Loss: 0.0061237 Loss Theta: 1.67012\n",
      "Epoch 293/300 Mapping Network Loss: 0.0060553 Loss Theta: 1.65673\n",
      "Epoch 294/300 Mapping Network Loss: 0.0060568 Loss Theta: 1.64107\n",
      "Epoch 295/300 Mapping Network Loss: 0.0060675 Loss Theta: 1.68421\n",
      "Epoch 296/300 Mapping Network Loss: 0.0060672 Loss Theta: 1.68414\n",
      "Epoch 297/300 Mapping Network Loss: 0.0060857 Loss Theta: 1.65968\n",
      "Epoch 298/300 Mapping Network Loss: 0.0060688 Loss Theta: 1.67228\n",
      "Epoch 299/300 Mapping Network Loss: 0.0060437 Loss Theta: 1.64898\n",
      "Epoch 300/300 Mapping Network Loss: 0.0060692 Loss Theta: 1.67463\n"
     ]
    }
   ],
   "source": [
    "mapping_net = MappingNetwork(cond_dim=3, latent_dim=latent_dim)\n",
    "\n",
    "for param in cdae.decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cdae.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "optimizer_mapping = optim.Adam(mapping_net.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "config_mapping = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"num_epochs\": 300,\n",
    "    \"lambda_theta\": 1e-3\n",
    "}\n",
    "\n",
    "\n",
    "num_epochs_mapping = config_mapping['num_epochs']\n",
    "\n",
    "lambda_theta = config_mapping['lambda_theta']\n",
    "\n",
    "mapping_net.train()\n",
    "cdae.eval()\n",
    "\n",
    "for epoch in range(num_epochs_mapping):\n",
    "    epoch_loss = 0.0\n",
    "    for cond, y in dataloader:\n",
    "        optimizer_mapping.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, latent = cdae(y, cond, add_noise=False)\n",
    "        \n",
    "        pred_latent = mapping_net(cond)\n",
    "        loss_map = criterion(pred_latent, latent)\n",
    "    \n",
    "        loss_theta = torch.norm(mapping_net.residual[0].weight, p=2)**2 + torch.norm(mapping_net.residual[2].weight, p=2)**2\n",
    "        \n",
    "        total_loss = loss_map + lambda_theta * loss_theta\n",
    "        total_loss.backward()\n",
    "        optimizer_mapping.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item() * cond.size(0)\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_mapping} Mapping Network Loss: {epoch_loss:.7f} Loss Theta: {loss_theta:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Final Model Loss: 0.04945\n",
      "Epoch 2/200 Final Model Loss: 0.04301\n",
      "Epoch 3/200 Final Model Loss: 0.03883\n",
      "Epoch 4/200 Final Model Loss: 0.03536\n",
      "Epoch 5/200 Final Model Loss: 0.03244\n",
      "Epoch 6/200 Final Model Loss: 0.02891\n",
      "Epoch 7/200 Final Model Loss: 0.02582\n",
      "Epoch 8/200 Final Model Loss: 0.02440\n",
      "Epoch 9/200 Final Model Loss: 0.02178\n",
      "Epoch 10/200 Final Model Loss: 0.02026\n",
      "Epoch 11/200 Final Model Loss: 0.01828\n",
      "Epoch 12/200 Final Model Loss: 0.01683\n",
      "Epoch 13/200 Final Model Loss: 0.01544\n",
      "Epoch 14/200 Final Model Loss: 0.01452\n",
      "Epoch 15/200 Final Model Loss: 0.01387\n",
      "Epoch 16/200 Final Model Loss: 0.01396\n",
      "Epoch 17/200 Final Model Loss: 0.01166\n",
      "Epoch 18/200 Final Model Loss: 0.01142\n",
      "Epoch 19/200 Final Model Loss: 0.00994\n",
      "Epoch 20/200 Final Model Loss: 0.00941\n",
      "Epoch 21/200 Final Model Loss: 0.00880\n",
      "Epoch 22/200 Final Model Loss: 0.00853\n",
      "Epoch 23/200 Final Model Loss: 0.00841\n",
      "Epoch 24/200 Final Model Loss: 0.00813\n",
      "Epoch 25/200 Final Model Loss: 0.00744\n",
      "Epoch 26/200 Final Model Loss: 0.00691\n",
      "Epoch 27/200 Final Model Loss: 0.00640\n",
      "Epoch 28/200 Final Model Loss: 0.00738\n",
      "Epoch 29/200 Final Model Loss: 0.00630\n",
      "Epoch 30/200 Final Model Loss: 0.00772\n",
      "Epoch 31/200 Final Model Loss: 0.00652\n",
      "Epoch 32/200 Final Model Loss: 0.00591\n",
      "Epoch 33/200 Final Model Loss: 0.00597\n",
      "Epoch 34/200 Final Model Loss: 0.00508\n",
      "Epoch 35/200 Final Model Loss: 0.00498\n",
      "Epoch 36/200 Final Model Loss: 0.00502\n",
      "Epoch 37/200 Final Model Loss: 0.00528\n",
      "Epoch 38/200 Final Model Loss: 0.00447\n",
      "Epoch 39/200 Final Model Loss: 0.00502\n",
      "Epoch 40/200 Final Model Loss: 0.00512\n",
      "Epoch 41/200 Final Model Loss: 0.00429\n",
      "Epoch 42/200 Final Model Loss: 0.00480\n",
      "Epoch 43/200 Final Model Loss: 0.00471\n",
      "Epoch 44/200 Final Model Loss: 0.00417\n",
      "Epoch 45/200 Final Model Loss: 0.00464\n",
      "Epoch 46/200 Final Model Loss: 0.00376\n",
      "Epoch 47/200 Final Model Loss: 0.00475\n",
      "Epoch 48/200 Final Model Loss: 0.00458\n",
      "Epoch 49/200 Final Model Loss: 0.00478\n",
      "Epoch 50/200 Final Model Loss: 0.00435\n",
      "Epoch 51/200 Final Model Loss: 0.00456\n",
      "Epoch 52/200 Final Model Loss: 0.00412\n",
      "Epoch 53/200 Final Model Loss: 0.00378\n",
      "Epoch 54/200 Final Model Loss: 0.00354\n",
      "Epoch 55/200 Final Model Loss: 0.00369\n",
      "Epoch 56/200 Final Model Loss: 0.00440\n",
      "Epoch 57/200 Final Model Loss: 0.00333\n",
      "Epoch 58/200 Final Model Loss: 0.00300\n",
      "Epoch 59/200 Final Model Loss: 0.00348\n",
      "Epoch 60/200 Final Model Loss: 0.00349\n",
      "Epoch 61/200 Final Model Loss: 0.00328\n",
      "Epoch 62/200 Final Model Loss: 0.00325\n",
      "Epoch 63/200 Final Model Loss: 0.00415\n",
      "Epoch 64/200 Final Model Loss: 0.00314\n",
      "Epoch 65/200 Final Model Loss: 0.00349\n",
      "Epoch 66/200 Final Model Loss: 0.00304\n",
      "Epoch 67/200 Final Model Loss: 0.00301\n",
      "Epoch 68/200 Final Model Loss: 0.00319\n",
      "Epoch 69/200 Final Model Loss: 0.00314\n",
      "Epoch 70/200 Final Model Loss: 0.00448\n",
      "Epoch 71/200 Final Model Loss: 0.00307\n",
      "Epoch 72/200 Final Model Loss: 0.00351\n",
      "Epoch 73/200 Final Model Loss: 0.00516\n",
      "Epoch 74/200 Final Model Loss: 0.00259\n",
      "Epoch 75/200 Final Model Loss: 0.00272\n",
      "Epoch 76/200 Final Model Loss: 0.00294\n",
      "Epoch 77/200 Final Model Loss: 0.00519\n",
      "Epoch 78/200 Final Model Loss: 0.00516\n",
      "Epoch 79/200 Final Model Loss: 0.00406\n",
      "Epoch 80/200 Final Model Loss: 0.00292\n",
      "Epoch 81/200 Final Model Loss: 0.00264\n",
      "Epoch 82/200 Final Model Loss: 0.00284\n",
      "Epoch 83/200 Final Model Loss: 0.00303\n",
      "Epoch 84/200 Final Model Loss: 0.00267\n",
      "Epoch 85/200 Final Model Loss: 0.00217\n",
      "Epoch 86/200 Final Model Loss: 0.00280\n",
      "Epoch 87/200 Final Model Loss: 0.00322\n",
      "Epoch 88/200 Final Model Loss: 0.00343\n",
      "Epoch 89/200 Final Model Loss: 0.00305\n",
      "Epoch 90/200 Final Model Loss: 0.00272\n",
      "Epoch 91/200 Final Model Loss: 0.00281\n",
      "Epoch 92/200 Final Model Loss: 0.00272\n",
      "Epoch 93/200 Final Model Loss: 0.00213\n",
      "Epoch 94/200 Final Model Loss: 0.00316\n",
      "Epoch 95/200 Final Model Loss: 0.00247\n",
      "Epoch 96/200 Final Model Loss: 0.00238\n",
      "Epoch 97/200 Final Model Loss: 0.00278\n",
      "Epoch 98/200 Final Model Loss: 0.00216\n",
      "Epoch 99/200 Final Model Loss: 0.00381\n",
      "Epoch 100/200 Final Model Loss: 0.00214\n",
      "Epoch 101/200 Final Model Loss: 0.00260\n",
      "Epoch 102/200 Final Model Loss: 0.00209\n",
      "Epoch 103/200 Final Model Loss: 0.00198\n",
      "Epoch 104/200 Final Model Loss: 0.00219\n",
      "Epoch 105/200 Final Model Loss: 0.00364\n",
      "Epoch 106/200 Final Model Loss: 0.00297\n",
      "Epoch 107/200 Final Model Loss: 0.00267\n",
      "Epoch 108/200 Final Model Loss: 0.00281\n",
      "Epoch 109/200 Final Model Loss: 0.00178\n",
      "Epoch 110/200 Final Model Loss: 0.00226\n",
      "Epoch 111/200 Final Model Loss: 0.00175\n",
      "Epoch 112/200 Final Model Loss: 0.00170\n",
      "Epoch 113/200 Final Model Loss: 0.00164\n",
      "Epoch 114/200 Final Model Loss: 0.00251\n",
      "Epoch 115/200 Final Model Loss: 0.00263\n",
      "Epoch 116/200 Final Model Loss: 0.00436\n",
      "Epoch 117/200 Final Model Loss: 0.00227\n",
      "Epoch 118/200 Final Model Loss: 0.00179\n",
      "Epoch 119/200 Final Model Loss: 0.00232\n",
      "Epoch 120/200 Final Model Loss: 0.00365\n",
      "Epoch 121/200 Final Model Loss: 0.00258\n",
      "Epoch 122/200 Final Model Loss: 0.00213\n",
      "Epoch 123/200 Final Model Loss: 0.00181\n",
      "Epoch 124/200 Final Model Loss: 0.00249\n",
      "Epoch 125/200 Final Model Loss: 0.00191\n",
      "Epoch 126/200 Final Model Loss: 0.00142\n",
      "Epoch 127/200 Final Model Loss: 0.00160\n",
      "Epoch 128/200 Final Model Loss: 0.00234\n",
      "Epoch 129/200 Final Model Loss: 0.00417\n",
      "Epoch 130/200 Final Model Loss: 0.00223\n",
      "Epoch 131/200 Final Model Loss: 0.00194\n",
      "Epoch 132/200 Final Model Loss: 0.00228\n",
      "Epoch 133/200 Final Model Loss: 0.00164\n",
      "Epoch 134/200 Final Model Loss: 0.00274\n",
      "Epoch 135/200 Final Model Loss: 0.00201\n",
      "Epoch 136/200 Final Model Loss: 0.00166\n",
      "Epoch 137/200 Final Model Loss: 0.00181\n",
      "Epoch 138/200 Final Model Loss: 0.00245\n",
      "Epoch 139/200 Final Model Loss: 0.00198\n",
      "Epoch 140/200 Final Model Loss: 0.00250\n",
      "Epoch 141/200 Final Model Loss: 0.00221\n",
      "Epoch 142/200 Final Model Loss: 0.00166\n",
      "Epoch 143/200 Final Model Loss: 0.00153\n",
      "Epoch 144/200 Final Model Loss: 0.00153\n",
      "Epoch 145/200 Final Model Loss: 0.00250\n",
      "Epoch 146/200 Final Model Loss: 0.00391\n",
      "Epoch 147/200 Final Model Loss: 0.00250\n",
      "Epoch 148/200 Final Model Loss: 0.00135\n",
      "Epoch 149/200 Final Model Loss: 0.00244\n",
      "Epoch 150/200 Final Model Loss: 0.00183\n",
      "Epoch 151/200 Final Model Loss: 0.00169\n",
      "Epoch 152/200 Final Model Loss: 0.00177\n",
      "Epoch 153/200 Final Model Loss: 0.00208\n",
      "Epoch 154/200 Final Model Loss: 0.00352\n",
      "Epoch 155/200 Final Model Loss: 0.00284\n",
      "Epoch 156/200 Final Model Loss: 0.00299\n",
      "Epoch 157/200 Final Model Loss: 0.00388\n",
      "Epoch 158/200 Final Model Loss: 0.00211\n",
      "Epoch 159/200 Final Model Loss: 0.00323\n",
      "Epoch 160/200 Final Model Loss: 0.00203\n",
      "Epoch 161/200 Final Model Loss: 0.00250\n",
      "Epoch 162/200 Final Model Loss: 0.00152\n",
      "Epoch 163/200 Final Model Loss: 0.00159\n",
      "Epoch 164/200 Final Model Loss: 0.00183\n",
      "Epoch 165/200 Final Model Loss: 0.00157\n",
      "Epoch 166/200 Final Model Loss: 0.00195\n",
      "Epoch 167/200 Final Model Loss: 0.00149\n",
      "Epoch 168/200 Final Model Loss: 0.00191\n",
      "Epoch 169/200 Final Model Loss: 0.00240\n",
      "Epoch 170/200 Final Model Loss: 0.00322\n",
      "Epoch 171/200 Final Model Loss: 0.00456\n",
      "Epoch 172/200 Final Model Loss: 0.00276\n",
      "Epoch 173/200 Final Model Loss: 0.00419\n",
      "Epoch 174/200 Final Model Loss: 0.00196\n",
      "Epoch 175/200 Final Model Loss: 0.00182\n",
      "Epoch 176/200 Final Model Loss: 0.00119\n",
      "Epoch 177/200 Final Model Loss: 0.00216\n",
      "Epoch 178/200 Final Model Loss: 0.00200\n",
      "Epoch 179/200 Final Model Loss: 0.00143\n",
      "Epoch 180/200 Final Model Loss: 0.00140\n",
      "Epoch 181/200 Final Model Loss: 0.00116\n",
      "Epoch 182/200 Final Model Loss: 0.00187\n",
      "Epoch 183/200 Final Model Loss: 0.00232\n",
      "Epoch 184/200 Final Model Loss: 0.00182\n",
      "Epoch 185/200 Final Model Loss: 0.00282\n",
      "Epoch 186/200 Final Model Loss: 0.00148\n",
      "Epoch 187/200 Final Model Loss: 0.00101\n",
      "Epoch 188/200 Final Model Loss: 0.00153\n",
      "Epoch 189/200 Final Model Loss: 0.00104\n",
      "Epoch 190/200 Final Model Loss: 0.00111\n",
      "Epoch 191/200 Final Model Loss: 0.00215\n",
      "Epoch 192/200 Final Model Loss: 0.00292\n",
      "Epoch 193/200 Final Model Loss: 0.00204\n",
      "Epoch 194/200 Final Model Loss: 0.00135\n",
      "Epoch 195/200 Final Model Loss: 0.00106\n",
      "Epoch 196/200 Final Model Loss: 0.00104\n",
      "Epoch 197/200 Final Model Loss: 0.00118\n",
      "Epoch 198/200 Final Model Loss: 0.00153\n",
      "Epoch 199/200 Final Model Loss: 0.00147\n",
      "Epoch 200/200 Final Model Loss: 0.00161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Final Predictor\n",
    "class FinalPredictor(nn.Module):\n",
    "    def __init__(self, mapping_net, decoder):\n",
    "        super(FinalPredictor, self).__init__()\n",
    "        self.mapping_net = mapping_net\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        pred_latent = self.mapping_net(cond)\n",
    "        \n",
    "        dec_input = torch.cat([pred_latent, cond], dim=1)\n",
    "        y_pred = self.decoder(dec_input)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "for param in cdae.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "final_model = FinalPredictor(mapping_net, cdae.decoder)\n",
    "\n",
    "optimizer_final = optim.Adam(final_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "config_final = {\n",
    "    \"num_epochs\":200, # 300\n",
    "    \"lr\": 1e-3,\n",
    "    \"p_encoder\": 0.0,\n",
    "    \"max_norm_clip\": 0.1\n",
    "}\n",
    "\n",
    "\n",
    "num_epochs_final = config_final['num_epochs']\n",
    "p_encoder = config_final['p_encoder']\n",
    "max_norm_clip = config_final['max_norm_clip']\n",
    "\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(num_epochs_final):\n",
    "    epoch_loss = 0.0\n",
    "    for cond, y in dataloader:\n",
    "        optimizer_final.zero_grad()\n",
    "        \n",
    "        if np.random.rand() < p_encoder:\n",
    "            with torch.no_grad():\n",
    "                _, latent = cdae(y, cond, add_noise=False)\n",
    "        else:\n",
    "            latent = final_model.mapping_net(cond)\n",
    "        \n",
    "        dec_input = torch.cat([latent, cond], dim=1)\n",
    "        y_pred = final_model.decoder(dec_input)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        ### clip loss\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm_clip)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        epoch_loss += loss.item() * cond.size(0)\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_final} Final Model Loss: {epoch_loss:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSQ Test Loss: 0.03636\n",
      "Test Loss: 0.00132\n"
     ]
    }
   ],
   "source": [
    "def evaluate_final_model(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_truth = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cond, y in dataloader:\n",
    "            y_pred = model(cond)\n",
    "            \n",
    "            all_preds.append(y_pred)\n",
    "            all_truth.append(y)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item() * cond.size(0)\n",
    "            total_samples += cond.size(0)\n",
    "            \n",
    "    avg_loss = total_loss / total_samples\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_truth = torch.cat(all_truth, dim=0)\n",
    "    \n",
    "    error_per_component = torch.abs(all_preds - all_truth)\n",
    "    max_error_per_component = torch.max(error_per_component, dim=0).values\n",
    "    min_error_per_component = torch.min(error_per_component, dim=0).values\n",
    "    median_error_per_component = torch.median(error_per_component, dim=0).values\n",
    "    \n",
    "    mse_per_component = torch.mean((all_preds - all_truth)**2, dim=0)\n",
    "    rsme_per_component = torch.sqrt(mse_per_component)\n",
    "\n",
    "    return avg_loss, rsme_per_component, max_error_per_component, min_error_per_component, median_error_per_component, error_per_component\n",
    "\n",
    "\n",
    "test_loss, rsme_per_component, max_per_component, min_per_component, median_per_component, error_per_component = evaluate_final_model(final_model, test_dataloader, criterion)\n",
    "print(f\"RMSQ Test Loss: {np.sqrt(test_loss):.5f}\")\n",
    "print(f\"Test Loss: {test_loss:.5f}\")\n",
    "# print(rsme_per_component)\n",
    "# print(max_per_component)\n",
    "# print(min_per_component)\n",
    "# print(median_per_component)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the final model: 5821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Number of parameters in the final model: {count_parameters(final_model)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
