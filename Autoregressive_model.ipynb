{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1079a7c10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 20)\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_3000_points.txt\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].split()\n",
    "    data[i] = [float(x) for x in data[i]]\n",
    "    \n",
    "data = np.array(data)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 3) (2400, 17)\n",
      "(300, 3) (300, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ratio_test_val_train = 0.20\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data[:, 0:3], data[:, 3:], test_size=ratio_test_val_train, random_state=seed+50)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed+50)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_train)\n",
    "y_scaled = scaler_Y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 3) (2400, 17)\n",
      "(300, 3) (300, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LTPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "dataset = LTPDataset(X_scaled, y_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "y_val_scaled = scaler_Y.transform(y_val)\n",
    "\n",
    "val_dataset = LTPDataset(X_val_scaled, y_val_scaled)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "X_scaled_test = scaler_X.transform(X_test)\n",
    "y_scaled_test = scaler_Y.transform(y_test)\n",
    "\n",
    "test_dataset = LTPDataset(X_scaled_test, y_scaled_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(X_scaled.shape, y_scaled.shape)\n",
    "print(X_scaled_test.shape, y_scaled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedBackbone(nn.Module):\n",
    "    def __init__(self, input_dim, shared_dim):\n",
    "        super(SharedBackbone, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, shared_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(shared_dim, shared_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class AutoregressiveHead(nn.Module):\n",
    "    def __init__(self, shared_dim, prev_dim, hidden_dim, dropout_rate=0.1):\n",
    "        super(AutoregressiveHead, self).__init__()\n",
    "        self.prev_dim = prev_dim\n",
    "        \n",
    "        # The input dimension for the MLP is shared features + previous outputs (if any)\n",
    "        mlp_input_dim = shared_dim + prev_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # Residual connection: project shared features to one output.\n",
    "        self.skip = nn.Linear(shared_dim, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, f, prev_y):\n",
    "        # If previous outputs exist, concatenate them with f.\n",
    "        if self.prev_dim > 0 and prev_y is not None:\n",
    "            combined_input = torch.cat([f, prev_y], dim=1)\n",
    "        else:\n",
    "            combined_input = f\n",
    "\n",
    "        mlp_out = self.mlp(combined_input)\n",
    "        skip_out = self.skip(f)\n",
    "        y_i_pred = mlp_out + skip_out  # Residual connection\n",
    "        return y_i_pred\n",
    "\n",
    "\n",
    "\n",
    "class AutoregressiveModelWithSharedBackbone(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_dim, head_hidden_dim, dropout_rate=0.1):\n",
    "        super(AutoregressiveModelWithSharedBackbone, self).__init__()\n",
    "        self.shared_backbone = SharedBackbone(input_dim, shared_dim)\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Create autoregressive heads.\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(output_dim):\n",
    "            self.heads.append(\n",
    "                AutoregressiveHead(shared_dim, prev_dim=i, hidden_dim=head_hidden_dim, dropout_rate=dropout_rate)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, y_true=None, teacher_forcing_ratio=1.0):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        f = self.shared_backbone(x)\n",
    "        y_preds = []\n",
    "        aux_outputs = []\n",
    "        \n",
    "        for i, head in enumerate(self.heads):\n",
    "            if i == 0:\n",
    "                prev_y = None\n",
    "            else:\n",
    "                prev_list = []\n",
    "                for j in range(i):\n",
    "                    if y_true is not None and self.training:\n",
    "                        # For each previous output, decide whether to use ground truth or model prediction.\n",
    "                        use_gt = torch.rand(batch_size, 1, device=x.device) < teacher_forcing_ratio\n",
    "                        prev_val = torch.where(use_gt, y_true[:, j:j+1], y_preds[j])\n",
    "                    else:\n",
    "                        prev_val = y_preds[j]\n",
    "                    prev_list.append(prev_val)\n",
    "                \n",
    "                prev_y = torch.cat(prev_list, dim=1)\n",
    "                \n",
    "            y_i_pred = head(f, prev_y)\n",
    "            aux_outputs.append(y_i_pred)\n",
    "            \n",
    "            y_preds.append(y_i_pred)\n",
    "            \n",
    "            \n",
    "        y_base = torch.cat(y_preds, dim=1)\n",
    "        return y_base, aux_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectionNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        \n",
    "        super(CorrectionNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y_base):\n",
    "        combined = torch.cat([x, y_base], dim=1)\n",
    "        correction = self.net(combined)\n",
    "        return correction\n",
    "\n",
    "\n",
    "class AutoregressiveWithCorrectionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_dim, head_hidden_dim, correction_hidden_dim, dropout_rate=0.1):\n",
    "        \n",
    "        super(AutoregressiveWithCorrectionModel, self).__init__()\n",
    "        self.auto_model = AutoregressiveModelWithSharedBackbone(input_dim, output_dim, shared_dim, head_hidden_dim, dropout_rate)\n",
    "        self.correction_net = CorrectionNet(input_dim, output_dim, correction_hidden_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, y_true=None, teacher_forcing_ratio=1.0):\n",
    "        # Base autoregressive prediction.\n",
    "        y_base, aux_outputs  = self.auto_model(x, y_true, teacher_forcing_ratio)\n",
    "        # Correction step.\n",
    "        correction = self.correction_net(x, y_base)\n",
    "        y_final = y_base + correction\n",
    "        return y_final , aux_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 3\n",
    "output_dim = 17\n",
    "shared_dim = 32\n",
    "head_hidden_dim = 32\n",
    "correction_hidden_dim = 32\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "model = AutoregressiveWithCorrectionModel(input_dim, output_dim, shared_dim, head_hidden_dim, correction_hidden_dim, dropout_rate)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Total Loss: 0.0202, Main Loss: 0.0097, Aux Loss: 0.0209, Teacher Forcing Ratio: 1.00\n",
      "Validation Loss after epoch 10: 0.0963\n",
      "Epoch [20/500], Total Loss: 0.0096, Main Loss: 0.0051, Aux Loss: 0.0090, Teacher Forcing Ratio: 1.00\n",
      "Validation Loss after epoch 20: 0.0632\n",
      "Epoch [30/500], Total Loss: 0.0050, Main Loss: 0.0031, Aux Loss: 0.0039, Teacher Forcing Ratio: 1.00\n",
      "Validation Loss after epoch 30: 0.0333\n",
      "Epoch [40/500], Total Loss: 0.0066, Main Loss: 0.0027, Aux Loss: 0.0077, Teacher Forcing Ratio: 0.99\n",
      "Validation Loss after epoch 40: 0.0180\n",
      "Epoch [50/500], Total Loss: 0.0068, Main Loss: 0.0036, Aux Loss: 0.0063, Teacher Forcing Ratio: 0.99\n",
      "Validation Loss after epoch 50: 0.0100\n",
      "Epoch [60/500], Total Loss: 0.0068, Main Loss: 0.0047, Aux Loss: 0.0042, Teacher Forcing Ratio: 0.98\n",
      "Validation Loss after epoch 60: 0.0103\n",
      "Epoch [70/500], Total Loss: 0.0045, Main Loss: 0.0022, Aux Loss: 0.0046, Teacher Forcing Ratio: 0.98\n",
      "Validation Loss after epoch 70: 0.0070\n",
      "Epoch [80/500], Total Loss: 0.0083, Main Loss: 0.0053, Aux Loss: 0.0059, Teacher Forcing Ratio: 0.97\n",
      "Validation Loss after epoch 80: 0.0590\n",
      "Epoch [90/500], Total Loss: 0.0020, Main Loss: 0.0012, Aux Loss: 0.0014, Teacher Forcing Ratio: 0.96\n",
      "Validation Loss after epoch 90: 0.0137\n",
      "Epoch [100/500], Total Loss: 0.0018, Main Loss: 0.0011, Aux Loss: 0.0014, Teacher Forcing Ratio: 0.95\n",
      "Validation Loss after epoch 100: 0.0043\n",
      "Epoch [110/500], Total Loss: 0.0017, Main Loss: 0.0011, Aux Loss: 0.0013, Teacher Forcing Ratio: 0.94\n",
      "Validation Loss after epoch 110: 0.0047\n",
      "Epoch [120/500], Total Loss: 0.0016, Main Loss: 0.0009, Aux Loss: 0.0013, Teacher Forcing Ratio: 0.93\n",
      "Validation Loss after epoch 120: 0.0090\n",
      "Epoch [130/500], Total Loss: 0.0016, Main Loss: 0.0010, Aux Loss: 0.0012, Teacher Forcing Ratio: 0.92\n",
      "Validation Loss after epoch 130: 0.0062\n",
      "Epoch [140/500], Total Loss: 0.0108, Main Loss: 0.0062, Aux Loss: 0.0093, Teacher Forcing Ratio: 0.91\n",
      "Validation Loss after epoch 140: 0.0150\n",
      "Epoch [150/500], Total Loss: 0.0043, Main Loss: 0.0025, Aux Loss: 0.0034, Teacher Forcing Ratio: 0.90\n",
      "Validation Loss after epoch 150: 0.0625\n",
      "Epoch [160/500], Total Loss: 0.0035, Main Loss: 0.0018, Aux Loss: 0.0034, Teacher Forcing Ratio: 0.89\n",
      "Validation Loss after epoch 160: 0.0159\n",
      "Epoch [170/500], Total Loss: 0.0039, Main Loss: 0.0022, Aux Loss: 0.0033, Teacher Forcing Ratio: 0.87\n",
      "Validation Loss after epoch 170: 0.0067\n",
      "Epoch [180/500], Total Loss: 0.0084, Main Loss: 0.0042, Aux Loss: 0.0085, Teacher Forcing Ratio: 0.86\n",
      "Validation Loss after epoch 180: 0.0898\n",
      "Epoch [190/500], Total Loss: 0.0043, Main Loss: 0.0027, Aux Loss: 0.0031, Teacher Forcing Ratio: 0.84\n",
      "Validation Loss after epoch 190: 0.0072\n",
      "Epoch [200/500], Total Loss: 0.0023, Main Loss: 0.0013, Aux Loss: 0.0020, Teacher Forcing Ratio: 0.83\n",
      "Validation Loss after epoch 200: 0.0029\n",
      "Epoch [210/500], Total Loss: 0.0041, Main Loss: 0.0025, Aux Loss: 0.0032, Teacher Forcing Ratio: 0.81\n",
      "Validation Loss after epoch 210: 0.0023\n",
      "Epoch [220/500], Total Loss: 0.0012, Main Loss: 0.0007, Aux Loss: 0.0009, Teacher Forcing Ratio: 0.80\n",
      "Validation Loss after epoch 220: 0.0034\n",
      "Epoch [230/500], Total Loss: 0.0027, Main Loss: 0.0017, Aux Loss: 0.0020, Teacher Forcing Ratio: 0.78\n",
      "Validation Loss after epoch 230: 0.0116\n",
      "Epoch [240/500], Total Loss: 0.0061, Main Loss: 0.0028, Aux Loss: 0.0065, Teacher Forcing Ratio: 0.77\n",
      "Validation Loss after epoch 240: 0.0091\n",
      "Epoch [250/500], Total Loss: 0.0012, Main Loss: 0.0008, Aux Loss: 0.0009, Teacher Forcing Ratio: 0.75\n",
      "Validation Loss after epoch 250: 0.0030\n",
      "Epoch [260/500], Total Loss: 0.0048, Main Loss: 0.0033, Aux Loss: 0.0031, Teacher Forcing Ratio: 0.74\n",
      "Validation Loss after epoch 260: 0.0036\n",
      "Epoch [270/500], Total Loss: 0.0052, Main Loss: 0.0031, Aux Loss: 0.0041, Teacher Forcing Ratio: 0.72\n",
      "Validation Loss after epoch 270: 0.0072\n",
      "Epoch [280/500], Total Loss: 0.0025, Main Loss: 0.0015, Aux Loss: 0.0019, Teacher Forcing Ratio: 0.70\n",
      "Validation Loss after epoch 280: 0.0078\n",
      "Epoch [290/500], Total Loss: 0.0171, Main Loss: 0.0102, Aux Loss: 0.0138, Teacher Forcing Ratio: 0.69\n",
      "Validation Loss after epoch 290: 0.0038\n",
      "Epoch [300/500], Total Loss: 0.0013, Main Loss: 0.0008, Aux Loss: 0.0010, Teacher Forcing Ratio: 0.67\n",
      "Validation Loss after epoch 300: 0.0042\n",
      "Epoch [310/500], Total Loss: 0.0023, Main Loss: 0.0012, Aux Loss: 0.0022, Teacher Forcing Ratio: 0.66\n",
      "Validation Loss after epoch 310: 0.0017\n",
      "Epoch [320/500], Total Loss: 0.0014, Main Loss: 0.0008, Aux Loss: 0.0012, Teacher Forcing Ratio: 0.64\n",
      "Validation Loss after epoch 320: 0.0053\n",
      "Epoch [330/500], Total Loss: 0.0015, Main Loss: 0.0008, Aux Loss: 0.0014, Teacher Forcing Ratio: 0.63\n",
      "Validation Loss after epoch 330: 0.0070\n",
      "Epoch [340/500], Total Loss: 0.0030, Main Loss: 0.0018, Aux Loss: 0.0023, Teacher Forcing Ratio: 0.62\n",
      "Validation Loss after epoch 340: 0.0103\n",
      "Epoch [350/500], Total Loss: 0.0024, Main Loss: 0.0016, Aux Loss: 0.0016, Teacher Forcing Ratio: 0.60\n",
      "Validation Loss after epoch 350: 0.0079\n",
      "Epoch [360/500], Total Loss: 0.0033, Main Loss: 0.0021, Aux Loss: 0.0024, Teacher Forcing Ratio: 0.59\n",
      "Validation Loss after epoch 360: 0.0088\n",
      "Epoch [370/500], Total Loss: 0.0007, Main Loss: 0.0005, Aux Loss: 0.0006, Teacher Forcing Ratio: 0.58\n",
      "Validation Loss after epoch 370: 0.0066\n",
      "Epoch [380/500], Total Loss: 0.0039, Main Loss: 0.0020, Aux Loss: 0.0037, Teacher Forcing Ratio: 0.57\n",
      "Validation Loss after epoch 380: 0.0021\n",
      "Epoch [390/500], Total Loss: 0.0024, Main Loss: 0.0014, Aux Loss: 0.0019, Teacher Forcing Ratio: 0.56\n",
      "Validation Loss after epoch 390: 0.0058\n",
      "Epoch [400/500], Total Loss: 0.0018, Main Loss: 0.0010, Aux Loss: 0.0016, Teacher Forcing Ratio: 0.55\n",
      "Validation Loss after epoch 400: 0.0020\n",
      "Epoch [410/500], Total Loss: 0.0032, Main Loss: 0.0015, Aux Loss: 0.0033, Teacher Forcing Ratio: 0.54\n",
      "Validation Loss after epoch 410: 0.0040\n",
      "Epoch [420/500], Total Loss: 0.0022, Main Loss: 0.0013, Aux Loss: 0.0017, Teacher Forcing Ratio: 0.53\n",
      "Validation Loss after epoch 420: 0.0034\n",
      "Epoch [430/500], Total Loss: 0.0051, Main Loss: 0.0030, Aux Loss: 0.0043, Teacher Forcing Ratio: 0.52\n",
      "Validation Loss after epoch 430: 0.0037\n",
      "Epoch [440/500], Total Loss: 0.0013, Main Loss: 0.0007, Aux Loss: 0.0011, Teacher Forcing Ratio: 0.52\n",
      "Validation Loss after epoch 440: 0.0096\n",
      "Epoch [450/500], Total Loss: 0.0041, Main Loss: 0.0023, Aux Loss: 0.0035, Teacher Forcing Ratio: 0.51\n",
      "Validation Loss after epoch 450: 0.0109\n",
      "Epoch [460/500], Total Loss: 0.0014, Main Loss: 0.0008, Aux Loss: 0.0011, Teacher Forcing Ratio: 0.51\n",
      "Validation Loss after epoch 460: 0.0044\n",
      "Epoch [470/500], Total Loss: 0.0010, Main Loss: 0.0006, Aux Loss: 0.0008, Teacher Forcing Ratio: 0.50\n",
      "Validation Loss after epoch 470: 0.0140\n",
      "Epoch [480/500], Total Loss: 0.0031, Main Loss: 0.0021, Aux Loss: 0.0020, Teacher Forcing Ratio: 0.50\n",
      "Validation Loss after epoch 480: 0.0156\n",
      "Epoch [490/500], Total Loss: 0.0020, Main Loss: 0.0011, Aux Loss: 0.0017, Teacher Forcing Ratio: 0.50\n",
      "Validation Loss after epoch 490: 0.0047\n",
      "Epoch [500/500], Total Loss: 0.0028, Main Loss: 0.0016, Aux Loss: 0.0023, Teacher Forcing Ratio: 0.50\n",
      "Validation Loss after epoch 500: 0.0082\n"
     ]
    }
   ],
   "source": [
    "initial_teacher_forcing_ratio = 1.0\n",
    "final_teacher_forcing_ratio = 0.5\n",
    "num_epochs = 500\n",
    "\n",
    "aux_loss_weight = 0.5\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # # Linearly decay teacher forcing ratio from initial to final over epochs.\n",
    "        # teacher_forcing_ratio = initial_teacher_forcing_ratio - \\\n",
    "        #     (initial_teacher_forcing_ratio - final_teacher_forcing_ratio) * (epoch / num_epochs)\n",
    "\n",
    "    # # Cosine decay of teacher forcing ratio from initial to final over epochs.\n",
    "    teacher_forcing_ratio = final_teacher_forcing_ratio + 0.5 * (initial_teacher_forcing_ratio - final_teacher_forcing_ratio) * \\\n",
    "                              (1 + np.cos(np.pi * epoch / num_epochs))\n",
    "    \n",
    "    model.train()\n",
    "    for i, (x, y_true) in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: get refined prediction and auxiliary outputs.\n",
    "        y_pred_refined, aux_outputs = model(x, y_true, teacher_forcing_ratio)\n",
    "\n",
    "\n",
    "        main_loss = criterion(y_pred_refined, y_true)\n",
    "\n",
    "        # Compute auxiliary losses for each intermediate y prediction.\n",
    "        aux_loss = 0\n",
    "        for i, aux_pred in enumerate(aux_outputs):\n",
    "            # Only compute auxiliary loss for the i-th output component.\n",
    "            aux_loss += criterion(aux_pred, y_true[:, i:i+1])\n",
    "        aux_loss = aux_loss / output_dim  # Average auxiliary loss\n",
    "\n",
    "        # Total loss: combine main and auxiliary losses.\n",
    "        loss = main_loss + aux_loss_weight * aux_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Total Loss: {loss.item():.4f}, \"\n",
    "            f\"Main Loss: {main_loss.item():.4f}, Aux Loss: {aux_loss.item():.4f}, \"\n",
    "            f\"Teacher Forcing Ratio: {teacher_forcing_ratio:.2f}\")\n",
    "\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val_true in val_dataloader:\n",
    "                y_val_pred_refined, _ = model(x_val)\n",
    "                val_loss += criterion(y_val_pred_refined, y_val_true).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Validation Loss after epoch {epoch+1}: {val_loss:.4f}\")\n",
    "        x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.0021375513169914484\n",
      "RMSE:  0.0462336599999551\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### evaluation\n",
    "model.eval()\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y_true in test_dataloader:\n",
    "        y_pred_refined, _ = model(x)\n",
    "        y_preds.append(y_pred_refined)\n",
    "        y_trues.append(y_true)\n",
    "y_preds = torch.cat(y_preds, dim=0)\n",
    "y_trues = torch.cat(y_trues, dim=0)\n",
    "\n",
    "loss = criterion(y_preds, y_trues)\n",
    "\n",
    "print(\"Test Loss: \", loss.item())\n",
    "print(\"RMSE: \", np.sqrt(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_model.shared_backbone.net.0.weight: tensor([[ 0.5450,  0.2920, -0.2309],\n",
      "        [ 0.5153, -0.0698,  0.0487],\n",
      "        [-0.3978,  0.3297,  0.5211],\n",
      "        [-0.6531,  0.0685,  0.0027],\n",
      "        [ 0.3838,  0.2231,  0.3660],\n",
      "        [ 0.0016,  0.1893,  0.1357],\n",
      "        [-0.2214,  0.1452, -0.4412],\n",
      "        [-0.1719, -0.1572,  0.4729],\n",
      "        [-0.6617, -0.2770, -0.1070],\n",
      "        [-0.2260,  0.0756, -0.6016],\n",
      "        [ 0.4219, -0.8554,  0.0398],\n",
      "        [ 0.3663, -0.2882,  0.2826],\n",
      "        [ 0.1709,  0.3195, -0.2946],\n",
      "        [-0.2114,  0.0360, -0.2896],\n",
      "        [ 0.1792,  0.2639,  0.3502],\n",
      "        [-0.2697,  0.1832,  0.0942],\n",
      "        [ 0.0541, -0.1987, -0.3901],\n",
      "        [-0.1562, -0.6272,  0.0830],\n",
      "        [ 0.4841,  0.2932,  0.0039],\n",
      "        [ 0.0712,  0.3373, -0.6338],\n",
      "        [ 0.1934, -0.3823, -0.0157],\n",
      "        [-0.1528,  0.0990, -0.3779],\n",
      "        [ 0.3878, -0.3137, -0.3665],\n",
      "        [-0.5842,  0.0983,  0.0514],\n",
      "        [ 0.4659, -0.3707, -0.6158],\n",
      "        [-0.5811, -0.4111,  0.1745],\n",
      "        [ 0.3624,  0.2522, -0.5433],\n",
      "        [-0.3178,  0.2222, -0.3607],\n",
      "        [ 0.4819, -0.2322,  0.2323],\n",
      "        [-0.6383, -0.1579,  0.2396],\n",
      "        [ 0.1482, -0.2885,  0.3907],\n",
      "        [ 0.1831,  0.0270, -0.6851]])\n",
      "auto_model.shared_backbone.net.0.bias: tensor([ 0.7775,  0.3058,  0.0263, -0.9347,  0.1845,  0.0592, -0.2306, -0.3860,\n",
      "        -0.1622, -0.7675, -0.7369, -0.5037, -0.6958, -0.6725,  0.9764,  0.0207,\n",
      "        -0.2794, -0.4493,  0.0499, -0.4575,  0.1332, -0.7446, -0.3401, -0.6243,\n",
      "        -0.0268,  0.0506, -0.1298, -0.6921, -0.0463, -0.3590,  0.4618, -0.7566])\n",
      "auto_model.shared_backbone.net.2.weight: tensor([[ 0.0304, -0.1150,  0.0593,  ...,  0.0146,  0.2365, -0.7023],\n",
      "        [ 0.2883, -0.1113, -0.1344,  ..., -0.0963, -0.2392,  0.1092],\n",
      "        [ 0.1909, -0.0550, -0.3133,  ..., -0.3480,  0.1922,  0.4403],\n",
      "        ...,\n",
      "        [ 0.1247,  0.0270, -0.1011,  ...,  0.2172, -0.2571,  0.3670],\n",
      "        [-0.1536,  0.1655,  0.0782,  ...,  0.1826,  0.4198,  0.1021],\n",
      "        [-0.0839,  0.0202,  0.0082,  ...,  0.4096, -0.3140, -0.1272]])\n",
      "auto_model.shared_backbone.net.2.bias: tensor([ 0.3582,  0.3512, -0.2676, -0.1912,  0.0985,  0.4686, -0.1469,  0.4583,\n",
      "         0.3239, -0.2137,  0.1461,  0.0512, -0.3441,  0.5555, -0.1201,  0.1868,\n",
      "        -0.3932, -0.1319,  0.3528,  0.0836,  0.0542, -0.2948,  0.1021,  0.5399,\n",
      "         0.0513,  0.6083, -0.1155,  0.1450,  0.0237,  0.0064, -0.3728,  0.0817])\n",
      "auto_model.heads.0.mlp.0.weight: tensor([[-0.0648, -0.0969, -0.0323,  ...,  0.0753, -0.2361,  0.0752],\n",
      "        [ 0.0942, -0.1972,  0.3441,  ..., -1.1483, -0.0077,  0.3219],\n",
      "        [-0.0204, -0.2570, -2.3811,  ..., -0.9525, -0.2352,  0.0141],\n",
      "        ...,\n",
      "        [-0.0619,  0.0300,  0.0961,  ..., -0.2411,  0.2582,  0.3583],\n",
      "        [ 0.0746,  0.2492,  0.2219,  ..., -1.8269,  0.2233,  0.0310],\n",
      "        [ 0.0738,  0.0388, -0.0521,  ...,  0.0431, -0.0153,  0.1239]])\n",
      "auto_model.heads.0.mlp.0.bias: tensor([ 0.0329, -0.2756, -0.1378,  0.0600, -0.1399, -0.0783, -0.1651,  0.1321,\n",
      "        -0.1862, -0.1549, -0.1587,  0.1941,  0.0851,  0.1689, -0.0351, -0.0507,\n",
      "         0.0517, -0.0799, -0.0278,  0.2233,  0.2546, -0.2657, -0.1273, -0.1755,\n",
      "        -0.0541, -0.1871,  0.0321, -0.2847,  0.1010, -0.0899, -0.3046, -0.1828])\n",
      "auto_model.heads.0.mlp.3.weight: tensor([[-0.0188,  0.1401,  0.0254, -0.0312,  0.0843, -0.0912, -0.0423, -0.1387,\n",
      "          0.1430, -0.0046, -0.0999, -0.1838, -0.0121, -0.0543,  0.0019,  0.0728,\n",
      "         -0.0495, -0.0070,  0.0074, -0.1587, -0.0427,  0.0121,  0.1004,  0.0020,\n",
      "          0.0572, -0.0548, -0.0143, -0.1106,  0.2066,  0.0601,  0.1325,  0.1008]])\n",
      "auto_model.heads.0.mlp.3.bias: tensor([0.1012])\n",
      "auto_model.heads.0.skip.weight: tensor([[ 0.1494,  0.1051,  0.0913,  0.2562, -0.0355, -0.0512, -0.1438, -0.0563,\n",
      "          0.2646, -0.2872,  0.2009,  0.3522, -0.0457,  0.1368, -0.0671, -0.0271,\n",
      "         -0.0023,  0.3969, -0.4894, -0.4069, -0.0098,  0.0265,  0.0488,  0.3568,\n",
      "         -0.0966, -0.5027,  0.0070, -0.4884, -0.0668, -0.1605,  0.1165, -0.3532]])\n",
      "auto_model.heads.0.skip.bias: tensor([-0.1837])\n",
      "auto_model.heads.1.mlp.0.weight: tensor([[-0.2923, -0.0141,  0.1410,  ..., -0.8555,  0.1680,  0.0768],\n",
      "        [-0.0543,  0.2106,  0.1617,  ..., -0.6978,  0.3291,  0.0713],\n",
      "        [-0.1397, -0.0562, -0.0466,  ..., -0.0716,  0.1045,  0.0570],\n",
      "        ...,\n",
      "        [-0.0979, -0.0542, -0.0011,  ...,  0.3600,  0.0120,  0.1798],\n",
      "        [-0.2157,  0.3102, -0.4875,  ..., -0.9344,  0.0105,  0.2575],\n",
      "        [-0.2562,  0.2387, -0.2110,  ..., -0.2342,  0.0124, -0.3538]])\n",
      "auto_model.heads.1.mlp.0.bias: tensor([ 0.1207,  0.0391, -0.1431, -0.0086, -0.0086, -0.1424, -0.0966, -0.1230,\n",
      "        -0.0269, -0.1936, -0.0955, -0.1211,  0.1243, -0.2065, -0.1102,  0.0525,\n",
      "         0.0475, -0.1213, -0.1125, -0.2094, -0.1078,  0.1324, -0.2162, -0.2831,\n",
      "        -0.1886, -0.1439, -0.0586, -0.1134, -0.0580, -0.1932,  0.1846, -0.1825])\n",
      "auto_model.heads.1.mlp.3.weight: tensor([[ 0.0447,  0.0382,  0.0203,  0.1050, -0.1089,  0.0814, -0.0348,  0.0553,\n",
      "          0.1103, -0.0581,  0.1149,  0.0693,  0.0708, -0.0222, -0.0007, -0.0337,\n",
      "         -0.1376,  0.1175, -0.2103, -0.0616, -0.0613,  0.0552,  0.0408, -0.2208,\n",
      "         -0.0293, -0.0861,  0.0883,  0.0460,  0.0502, -0.1710,  0.1182, -0.0980]])\n",
      "auto_model.heads.1.mlp.3.bias: tensor([0.2273])\n",
      "auto_model.heads.1.skip.weight: tensor([[-0.2360,  0.4113, -0.0884, -0.1682, -0.0866,  0.3285,  0.0415,  0.2507,\n",
      "         -0.3370,  0.0083,  0.1333,  0.0828, -0.2005, -0.1549, -0.1396, -0.0040,\n",
      "          0.0342,  0.2449,  0.0339, -0.0087,  0.1058, -0.1961,  0.1987,  0.1558,\n",
      "         -0.4112, -0.1570, -0.2360, -0.6884,  0.1755, -0.3475, -0.2141, -0.3101]])\n",
      "auto_model.heads.1.skip.bias: tensor([0.1624])\n",
      "auto_model.heads.2.mlp.0.weight: tensor([[ 0.0295,  0.1077,  0.0102,  ..., -0.0773,  0.2937, -0.2556],\n",
      "        [ 0.1835, -0.1056,  0.2877,  ..., -0.5433,  0.0346,  0.2801],\n",
      "        [ 0.0439, -0.0712,  0.0055,  ..., -0.3215, -0.0842,  0.4352],\n",
      "        ...,\n",
      "        [ 0.1184,  0.3438, -0.2924,  ..., -0.2410, -0.0391, -0.2105],\n",
      "        [ 0.2541, -0.1811,  0.3868,  ...,  0.0781, -0.2773,  0.5497],\n",
      "        [ 0.0024, -0.0416, -0.1567,  ...,  0.1827,  0.3719, -0.1883]])\n",
      "auto_model.heads.2.mlp.0.bias: tensor([-0.1645, -0.0226,  0.1840, -0.3144, -0.0412,  0.1566,  0.0317,  0.0956,\n",
      "        -0.1196, -0.0475, -0.1258, -0.0270, -0.1134, -0.0259,  0.1054, -0.0398,\n",
      "         0.1504,  0.1031,  0.0013, -0.0839,  0.0977, -0.3317,  0.1796, -0.1275,\n",
      "         0.1341, -0.1883,  0.1434, -0.2749, -0.0459,  0.0516, -0.0552, -0.1496])\n",
      "auto_model.heads.2.mlp.3.weight: tensor([[-0.0901,  0.2004,  0.0568, -0.1142,  0.0847, -0.1687, -0.1156, -0.0736,\n",
      "         -0.1475,  0.0997,  0.1895,  0.0469, -0.2405, -0.1935, -0.0434, -0.0875,\n",
      "          0.0945,  0.0744,  0.2044, -0.1859, -0.0553, -0.1295, -0.0586,  0.1292,\n",
      "         -0.0409,  0.1223, -0.1109, -0.1233, -0.0097, -0.0537,  0.1805, -0.1609]])\n",
      "auto_model.heads.2.mlp.3.bias: tensor([-0.0092])\n",
      "auto_model.heads.2.skip.weight: tensor([[-0.0650,  0.4195,  0.0690, -0.4843,  0.0524,  0.1254, -0.0810,  0.4057,\n",
      "         -0.4621,  0.2858,  0.3550, -0.1686,  0.0497,  0.0568, -0.0062,  0.0791,\n",
      "          0.1014,  0.1901,  0.2562, -0.1901, -0.2376, -0.7183, -0.0032,  0.1154,\n",
      "         -0.0042, -0.1610, -0.2125, -0.2944,  0.4931,  0.1328, -0.2720, -0.3676]])\n",
      "auto_model.heads.2.skip.bias: tensor([0.1336])\n",
      "auto_model.heads.3.mlp.0.weight: tensor([[-0.5599,  0.0998,  0.1672,  ..., -0.2805,  0.2454,  0.0141],\n",
      "        [-0.2145,  0.0968, -0.0970,  ...,  0.0921, -0.3504, -0.3690],\n",
      "        [-0.0352,  0.0244,  0.0263,  ..., -0.2279,  0.3155, -0.0371],\n",
      "        ...,\n",
      "        [-0.2886, -0.4192,  0.0223,  ..., -0.1438, -0.4034, -0.1795],\n",
      "        [-0.1634,  0.4177,  0.3294,  ...,  0.2261, -0.2306, -0.2205],\n",
      "        [-0.1342, -0.3123,  0.1681,  ..., -0.0321, -0.4626, -0.2180]])\n",
      "auto_model.heads.3.mlp.0.bias: tensor([-0.2929, -0.2588,  0.0362, -0.1582, -0.0570, -0.0747, -0.0350,  0.1388,\n",
      "         0.1040, -0.0805, -0.1793, -0.0481,  0.1783, -0.1715, -0.0682, -0.2093,\n",
      "        -0.0028, -0.2957,  0.0119, -0.2464,  0.1038, -0.1672,  0.0044, -0.3724,\n",
      "        -0.2507,  0.0562, -0.2873, -0.0202, -0.2436, -0.5183,  0.0867, -0.3991])\n",
      "auto_model.heads.3.mlp.3.weight: tensor([[ 0.1499, -0.1829,  0.0999,  0.0560,  0.0529, -0.0917, -0.2594, -0.1591,\n",
      "          0.0564,  0.1012,  0.0894,  0.0486,  0.0353, -0.0396, -0.0795, -0.1564,\n",
      "          0.0952, -0.1315,  0.0665,  0.1223,  0.0734,  0.0920,  0.0533,  0.2331,\n",
      "         -0.0862, -0.0740,  0.0559,  0.0806, -0.1582, -0.3635, -0.1392, -0.2637]])\n",
      "auto_model.heads.3.mlp.3.bias: tensor([0.2080])\n",
      "auto_model.heads.3.skip.weight: tensor([[-0.3064,  0.4247,  0.0614, -0.1896,  0.2297,  0.0263, -0.1444,  0.3579,\n",
      "         -0.2213, -0.0793,  0.1841, -0.0648,  0.0896, -0.3141, -0.0041,  0.0774,\n",
      "         -0.0516,  0.1309,  0.0344, -0.0028,  0.1303, -0.1608,  0.0496, -0.0372,\n",
      "          0.0215, -0.2062, -0.1851,  0.0238,  0.4552, -0.1922, -0.2867,  0.0763]])\n",
      "auto_model.heads.3.skip.bias: tensor([0.1078])\n",
      "auto_model.heads.4.mlp.0.weight: tensor([[-0.4205, -0.2431,  0.0639,  ...,  0.0012,  0.2302,  0.4358],\n",
      "        [-0.3101,  0.1677,  0.0739,  ...,  0.1340,  0.0373, -0.2508],\n",
      "        [-0.3594, -0.0504,  0.0430,  ..., -0.1499,  0.1030,  0.2809],\n",
      "        ...,\n",
      "        [-0.4104,  0.3535,  0.0466,  ...,  0.1392, -0.2444, -0.3208],\n",
      "        [-0.1924,  0.1375, -0.0087,  ..., -0.0244, -0.0846, -0.4213],\n",
      "        [-0.1355, -0.1554, -0.0196,  ..., -0.1165, -0.1313,  0.3101]])\n",
      "auto_model.heads.4.mlp.0.bias: tensor([-0.0820,  0.0453, -0.0778, -0.2286,  0.1129,  0.0855, -0.1353, -0.2575,\n",
      "        -0.0891, -0.0722, -0.1144, -0.2324, -0.1402, -0.2807, -0.2037, -0.0664,\n",
      "        -0.2018, -0.3542, -0.2147, -0.1855, -0.2832,  0.0203,  0.0964, -0.2167,\n",
      "        -0.1629, -0.2753, -0.3296, -0.4910, -0.0837, -0.0416,  0.1400, -0.3762])\n",
      "auto_model.heads.4.mlp.3.weight: tensor([[ 0.0706, -0.0969,  0.0691,  0.2930, -0.0557, -0.1337, -0.1179,  0.0764,\n",
      "          0.0517, -0.0474,  0.0282, -0.1788,  0.0177,  0.0893, -0.1837, -0.0596,\n",
      "          0.0769,  0.0669,  0.0816, -0.0191,  0.0738, -0.1348, -0.1916,  0.1080,\n",
      "          0.0845,  0.1015,  0.1254,  0.2070, -0.2725, -0.1415, -0.1434,  0.2063]])\n",
      "auto_model.heads.4.mlp.3.bias: tensor([0.0947])\n",
      "auto_model.heads.4.skip.weight: tensor([[ 3.9487e-02, -4.1917e-04,  2.5363e-01, -2.0970e-01,  4.6412e-01,\n",
      "          4.6099e-02,  3.7066e-02,  9.3209e-02, -1.0055e-01,  4.8794e-02,\n",
      "          7.8779e-02, -2.9495e-01,  3.9604e-01, -3.5020e-01, -6.9139e-02,\n",
      "          2.9392e-01,  6.4369e-01, -1.1190e-01,  1.4943e-01, -5.4640e-02,\n",
      "          2.1034e-01, -1.9960e-01, -3.2024e-01, -5.1604e-02,  2.5545e-01,\n",
      "         -1.6969e-01, -1.1260e-01,  8.6231e-02,  4.8828e-01,  2.1547e-01,\n",
      "         -1.8841e-01, -1.3108e-02]])\n",
      "auto_model.heads.4.skip.bias: tensor([-0.0055])\n",
      "auto_model.heads.5.mlp.0.weight: tensor([[ 0.4132, -0.3281, -0.4001,  ...,  0.1410,  0.4332,  0.2205],\n",
      "        [-0.1735,  0.1037,  0.0519,  ...,  0.0161, -0.0364, -0.0348],\n",
      "        [-0.3638, -0.2347,  0.1854,  ..., -0.0677, -0.3689,  0.1838],\n",
      "        ...,\n",
      "        [-0.3410,  0.3948,  0.1354,  ..., -0.1150, -0.2304, -0.2586],\n",
      "        [ 0.1516,  0.1296, -0.0849,  ..., -0.2626, -0.1323, -0.1174],\n",
      "        [-0.1325,  0.1491,  0.5161,  ..., -0.2754, -0.1083, -0.3301]])\n",
      "auto_model.heads.5.mlp.0.bias: tensor([-0.0701, -0.0346, -0.2501, -0.1935, -0.3425, -0.1642, -0.2477, -0.0008,\n",
      "        -0.0929, -0.1117, -0.1543,  0.1135, -0.2188, -0.1148, -0.2241, -0.1775,\n",
      "         0.0384,  0.1936, -0.2357, -0.0328,  0.0582, -0.1029, -0.0245,  0.0052,\n",
      "        -0.0736,  0.0745, -0.0166, -0.2342, -0.0586,  0.0264, -0.0819, -0.1434])\n",
      "auto_model.heads.5.mlp.3.weight: tensor([[ 0.0486,  0.0729, -0.0784,  0.0799, -0.1233,  0.0523,  0.0733, -0.0951,\n",
      "          0.0648,  0.0475, -0.0441, -0.0502,  0.0529, -0.1407,  0.0299,  0.0196,\n",
      "         -0.0391, -0.1470,  0.0542,  0.0641,  0.0914, -0.1410, -0.0561,  0.0623,\n",
      "          0.0672,  0.0730,  0.0735,  0.0494, -0.0532, -0.0329, -0.0835, -0.0368]])\n",
      "auto_model.heads.5.mlp.3.bias: tensor([-0.2210])\n",
      "auto_model.heads.5.skip.weight: tensor([[ 0.0011,  0.1948,  0.2682, -0.1818,  0.4198,  0.1463, -0.0423,  0.2934,\n",
      "         -0.1642,  0.1046,  0.0993, -0.3414,  0.0767, -0.1554,  0.0341,  0.2501,\n",
      "          0.3088,  0.0470,  0.2871, -0.1303,  0.3079, -0.0436, -0.2255, -0.0910,\n",
      "         -0.0251, -0.1623, -0.0971, -0.0018,  0.4233, -0.0030, -0.2599, -0.0860]])\n",
      "auto_model.heads.5.skip.bias: tensor([-0.0210])\n",
      "auto_model.heads.6.mlp.0.weight: tensor([[-0.2008, -0.1441,  0.3253,  ..., -0.0068,  0.1997,  0.1876],\n",
      "        [-0.3252, -0.4208,  0.2368,  ...,  0.0184,  0.1908,  0.0318],\n",
      "        [-0.3121, -0.2789,  0.1187,  ...,  0.0578,  0.2188,  0.1874],\n",
      "        ...,\n",
      "        [-0.4444, -0.1306,  0.1612,  ...,  0.1820,  0.4243,  0.1290],\n",
      "        [ 0.1884,  0.3031, -0.5590,  ...,  0.0872,  0.2746,  0.1980],\n",
      "        [ 0.0335,  0.4283, -0.6981,  ...,  0.0489,  0.1042,  0.1135]])\n",
      "auto_model.heads.6.mlp.0.bias: tensor([-0.0931, -0.3574, -0.2214,  0.1447, -0.3511,  0.0594, -0.3217, -0.2728,\n",
      "        -0.3029, -0.2332, -0.2118, -0.1946, -0.2933, -0.3569, -0.3578, -0.0175,\n",
      "        -0.1061, -0.4343, -0.1088, -0.0061,  0.0525, -0.2814, -0.1641, -0.0174,\n",
      "        -0.0696, -0.4241, -0.1610, -0.2053, -0.1326, -0.1580,  0.1418,  0.1751])\n",
      "auto_model.heads.6.mlp.3.weight: tensor([[ 0.1071,  0.4078,  0.2240, -0.0589,  0.1133, -0.1141,  0.4372,  0.1271,\n",
      "          0.3113,  0.0908,  0.3297,  0.3453,  0.2431,  0.2762,  0.1840,  0.0016,\n",
      "          0.2517,  0.1724, -0.0360, -0.0397,  0.0209,  0.1036,  0.3906, -0.0383,\n",
      "         -0.1410,  0.2902, -0.0957, -0.1268,  0.1496,  0.1738,  0.1002,  0.1458]])\n",
      "auto_model.heads.6.mlp.3.bias: tensor([-0.0925])\n",
      "auto_model.heads.6.skip.weight: tensor([[ 0.0351, -0.2647,  0.0665, -0.1001,  0.4664, -0.0893, -0.0893, -0.0900,\n",
      "         -0.1770,  0.2814, -0.1375,  0.4494,  1.0580, -0.2650,  0.0533,  0.2974,\n",
      "          0.8636,  0.1396,  0.4005,  0.2324, -0.1786,  0.3243,  0.1498, -0.1644,\n",
      "          0.4746,  0.0036,  0.1451, -0.0508,  0.2500,  0.5625,  0.4435, -0.1627]])\n",
      "auto_model.heads.6.skip.bias: tensor([0.1625])\n",
      "auto_model.heads.7.mlp.0.weight: tensor([[-0.4284, -0.4596,  0.0122,  ..., -0.1666, -0.4701,  0.5066],\n",
      "        [-0.0103,  0.0128,  0.0947,  ..., -0.0746,  0.0456, -0.0393],\n",
      "        [-0.0297, -0.2131,  0.0460,  ...,  0.1595,  0.0934, -0.3128],\n",
      "        ...,\n",
      "        [-0.5300, -0.7075,  0.0746,  ..., -0.2026, -0.3036,  0.4679],\n",
      "        [ 0.2929, -0.2432,  0.0950,  ...,  0.1991,  0.0018, -0.2559],\n",
      "        [-0.1107, -0.0551,  0.1799,  ...,  0.1758,  0.1176, -0.0692]])\n",
      "auto_model.heads.7.mlp.0.bias: tensor([-0.0698, -0.1317,  0.1864, -0.1019, -0.2636, -0.0179, -0.2504,  0.1906,\n",
      "        -0.4085, -0.0914, -0.0770, -0.2273,  0.1651, -0.1964, -0.0971,  0.1592,\n",
      "        -0.0346,  0.0255, -0.1558, -0.2093, -0.2597, -0.0989, -0.1101,  0.1175,\n",
      "        -0.1980,  0.0872, -0.0763,  0.1346, -0.0882, -0.3024,  0.0444,  0.0502])\n",
      "auto_model.heads.7.mlp.3.weight: tensor([[ 0.2522,  0.0602, -0.0710,  0.3478,  0.3008,  0.2746,  0.2774, -0.0989,\n",
      "          0.3463,  0.5369,  0.0411,  0.0248, -0.1287,  0.3929,  0.2275, -0.0676,\n",
      "          0.1133,  0.1149, -0.0115, -0.0214,  0.2596,  0.1834, -0.0643, -0.0945,\n",
      "          0.2713, -0.0818, -0.5269, -0.0781,  0.1639,  0.3987, -0.1736, -0.0537]])\n",
      "auto_model.heads.7.mlp.3.bias: tensor([0.0878])\n",
      "auto_model.heads.7.skip.weight: tensor([[-0.0703, -0.2134, -0.3134, -0.0641,  0.2187, -0.1855, -0.1664, -0.1077,\n",
      "         -0.1854,  0.4590, -0.2517,  0.5413,  0.7290, -0.1749, -0.0728,  0.1532,\n",
      "          0.2921,  0.3547,  0.2338,  0.4549, -0.2557,  0.3980,  0.6212, -0.0027,\n",
      "          0.2593,  0.0039,  0.3868, -0.1334, -0.1152,  0.3468,  0.7379, -0.2798]])\n",
      "auto_model.heads.7.skip.bias: tensor([0.1726])\n",
      "auto_model.heads.8.mlp.0.weight: tensor([[ 0.0977, -0.0124,  0.0681,  ...,  0.0192, -0.0423, -0.4485],\n",
      "        [ 0.1550,  0.0772, -0.0279,  ...,  0.0450,  0.0544, -0.0874],\n",
      "        [-0.3370,  0.3160,  0.3255,  ..., -0.0199, -0.1376,  0.0446],\n",
      "        ...,\n",
      "        [-0.2734,  0.5691, -0.0170,  ..., -0.2692,  0.1245,  0.0084],\n",
      "        [-0.1534,  0.2717,  0.4076,  ..., -0.1701,  0.1257, -0.0278],\n",
      "        [-0.2992,  0.1382,  0.3130,  ..., -0.1323,  0.0168, -0.0228]])\n",
      "auto_model.heads.8.mlp.0.bias: tensor([-0.0763, -0.1029, -0.0462, -0.1869, -0.1305, -0.0388, -0.1706,  0.0947,\n",
      "        -0.0403,  0.0789, -0.2291, -0.0847, -0.1463,  0.0274,  0.0078, -0.2067,\n",
      "         0.0578,  0.0549, -0.1039, -0.2180, -0.2465,  0.0539, -0.0370, -0.3142,\n",
      "         0.0247, -0.0173,  0.0213, -0.0168,  0.1120, -0.2151, -0.1813,  0.0229])\n",
      "auto_model.heads.8.mlp.3.weight: tensor([[ 0.0392,  0.0222, -0.0485, -0.0780,  0.0391, -0.0016, -0.0500,  0.0958,\n",
      "          0.0431,  0.0516, -0.0167,  0.0515,  0.0841,  0.0194, -0.0161,  0.0694,\n",
      "         -0.0225,  0.0088,  0.0217,  0.1097, -0.0327, -0.0565,  0.0349, -0.0838,\n",
      "         -0.0138,  0.0428,  0.0328,  0.0246,  0.0556, -0.0863, -0.0821, -0.0486]])\n",
      "auto_model.heads.8.mlp.3.bias: tensor([-0.0268])\n",
      "auto_model.heads.8.skip.weight: tensor([[-0.2293,  0.3551,  0.1784, -0.1145,  0.2587,  0.0778, -0.0816,  0.2585,\n",
      "         -0.1740,  0.0342,  0.2602, -0.1282,  0.0437, -0.2632,  0.0996,  0.2193,\n",
      "          0.1191,  0.1843, -0.0280, -0.2042,  0.0731, -0.1858,  0.0775,  0.1130,\n",
      "         -0.0350, -0.1352, -0.1908, -0.0408,  0.4877, -0.1187, -0.2460, -0.0353]])\n",
      "auto_model.heads.8.skip.bias: tensor([0.0192])\n",
      "auto_model.heads.9.mlp.0.weight: tensor([[ 0.1328,  0.0424,  0.2504,  ...,  0.1268,  0.0197,  0.1087],\n",
      "        [-0.0385, -0.2865,  0.0878,  ...,  0.1305,  0.0279,  0.0395],\n",
      "        [-0.1523,  0.0171, -0.1067,  ..., -0.0452, -0.0658,  0.1809],\n",
      "        ...,\n",
      "        [-0.1840, -0.2735,  0.0275,  ..., -0.0981, -0.0783, -0.2364],\n",
      "        [ 0.2246, -0.0034, -0.3845,  ..., -0.0596,  0.0614,  0.0573],\n",
      "        [ 0.0046, -0.2508,  0.0997,  ...,  0.0542,  0.0206, -0.0448]])\n",
      "auto_model.heads.9.mlp.0.bias: tensor([-0.4089, -0.4476, -0.1494, -0.0515, -0.2501,  0.1217, -0.1977, -0.2130,\n",
      "         0.1812, -0.0323, -0.0487, -0.0755,  0.2459, -0.0832, -0.2040, -0.1742,\n",
      "        -0.0928, -0.0387, -0.3222, -0.4124, -0.1375, -0.1421, -0.1696, -0.1796,\n",
      "        -0.0825, -0.0803, -0.0346, -0.3070,  0.0257, -0.1498, -0.0758, -0.1949])\n",
      "auto_model.heads.9.mlp.3.weight: tensor([[ 0.3453,  0.6682,  0.1199,  0.1046,  0.1261, -0.0524, -0.0808,  0.0953,\n",
      "         -0.1116, -0.0668,  0.0444,  0.0669, -0.0819,  0.0855,  0.1010, -0.0534,\n",
      "          0.1815, -0.0583,  0.3230,  0.4172,  0.1408, -0.0612,  0.1863, -0.1227,\n",
      "         -0.0736,  0.1363,  0.1719,  0.4061,  0.0382, -0.0433, -0.1608,  0.2404]])\n",
      "auto_model.heads.9.mlp.3.bias: tensor([-0.0593])\n",
      "auto_model.heads.9.skip.weight: tensor([[ 0.1670,  0.1414,  0.3120,  0.3084, -0.0477, -0.0811,  0.0112,  0.0756,\n",
      "          0.2889, -0.2251,  0.4696,  0.1496,  0.0048,  0.0396, -0.0661, -0.1095,\n",
      "         -0.1222,  0.5878, -0.3605, -0.2810, -0.0442, -0.3076,  0.0214,  0.2007,\n",
      "          0.1209, -0.4953, -0.1111, -0.1900, -0.0047, -0.1667,  0.2604, -0.1741]])\n",
      "auto_model.heads.9.skip.bias: tensor([-0.1744])\n",
      "auto_model.heads.10.mlp.0.weight: tensor([[ 0.2523,  0.0895,  0.5521,  ...,  0.0437, -0.0313, -0.4490],\n",
      "        [-0.2635,  0.0193,  0.2871,  ...,  0.0868,  0.0850,  0.2835],\n",
      "        [-0.1041,  0.1521, -0.0471,  ..., -0.0968,  0.0408,  0.2870],\n",
      "        ...,\n",
      "        [ 0.1741,  0.0647,  0.2903,  ...,  0.0176,  0.0751, -0.5008],\n",
      "        [-0.1574, -0.2070, -0.1862,  ...,  0.0139,  0.1167,  0.0082],\n",
      "        [-0.2064,  0.0153,  0.0589,  ...,  0.0107, -0.0624,  0.2606]])\n",
      "auto_model.heads.10.mlp.0.bias: tensor([-0.0518, -0.3295, -0.1587,  0.0413, -0.1128,  0.0786, -0.3492, -0.0765,\n",
      "        -0.0851, -0.2763, -0.0465,  0.1925, -0.2687, -0.2784, -0.2602, -0.0073,\n",
      "        -0.0056, -0.0636,  0.0105, -0.3609, -0.0457, -0.0847, -0.1523, -0.0011,\n",
      "        -0.1453, -0.1524, -0.0763, -0.1786, -0.1140,  0.0384, -0.1947, -0.2542])\n",
      "auto_model.heads.10.mlp.3.weight: tensor([[-0.0721,  0.1878,  0.1327, -0.1033,  0.1122, -0.1035,  0.1557, -0.1270,\n",
      "          0.0718, -0.2460,  0.0721, -0.0640, -0.1329,  0.1257,  0.1028, -0.1024,\n",
      "          0.1956, -0.0594, -0.0649,  0.1539,  0.0830, -0.1566,  0.1180,  0.0650,\n",
      "         -0.0775,  0.0786,  0.0853,  0.0725, -0.0581, -0.0663, -0.0273,  0.0679]])\n",
      "auto_model.heads.10.mlp.3.bias: tensor([0.0379])\n",
      "auto_model.heads.10.skip.weight: tensor([[ 0.2736,  0.2139,  0.5169,  0.4026,  0.0491, -0.1913,  0.0552, -0.1319,\n",
      "          0.2943, -0.4774,  0.6060,  0.1188, -0.0321,  0.0038, -0.0016, -0.1588,\n",
      "         -0.2275,  0.3210, -0.3918, -0.2526, -0.1650,  0.0292,  0.1364,  0.2157,\n",
      "         -0.0967, -0.3436, -0.1300, -0.1385, -0.2161, -0.0059,  0.4153, -0.2450]])\n",
      "auto_model.heads.10.skip.bias: tensor([-0.2031])\n",
      "auto_model.heads.11.mlp.0.weight: tensor([[-0.1268,  0.5860, -0.0603,  ..., -0.3239, -0.0957,  0.1742],\n",
      "        [-0.0353,  0.0928, -0.1980,  ...,  0.0683,  0.0269,  0.2254],\n",
      "        [-0.2051, -0.0928, -0.3423,  ...,  0.1734, -0.0120, -0.2075],\n",
      "        ...,\n",
      "        [ 0.2853, -0.1401, -0.0075,  ..., -0.0647,  0.1310, -0.1896],\n",
      "        [-0.0319,  0.0372,  0.0904,  ..., -0.1286, -0.0758, -0.0579],\n",
      "        [ 0.0232, -0.3770,  0.0319,  ..., -0.0425,  0.0678, -0.0144]])\n",
      "auto_model.heads.11.mlp.0.bias: tensor([-0.0748,  0.0430, -0.0050, -0.0256, -0.1304,  0.0122, -0.2730,  0.1592,\n",
      "        -0.1369, -0.0036,  0.0495, -0.0529, -0.1400,  0.0076, -0.1517, -0.1432,\n",
      "         0.2649, -0.0581, -0.1707,  0.0878, -0.0169,  0.1382, -0.1062,  0.1243,\n",
      "        -0.0820, -0.2091,  0.1015, -0.1318,  0.0741, -0.1461, -0.0120, -0.2060])\n",
      "auto_model.heads.11.mlp.3.weight: tensor([[-0.0526, -0.0047,  0.0647, -0.0333,  0.0166,  0.0280,  0.0205, -0.0174,\n",
      "          0.0282,  0.0253, -0.0165,  0.0036, -0.0557, -0.0713,  0.0325, -0.1685,\n",
      "         -0.0981, -0.1199, -0.0418, -0.0724,  0.0180, -0.0344, -0.0480, -0.0137,\n",
      "          0.0175,  0.0133, -0.0361,  0.0629,  0.0815,  0.0237,  0.0037,  0.0645]])\n",
      "auto_model.heads.11.mlp.3.bias: tensor([-0.2618])\n",
      "auto_model.heads.11.skip.weight: tensor([[-0.2270,  0.2481, -0.0248, -0.0384,  0.0840,  0.2501,  0.0872, -0.0817,\n",
      "          0.1224,  0.3755, -0.0199,  0.0337, -0.0222,  0.0709,  0.1312,  0.5047,\n",
      "          0.1850,  0.0387,  0.3553, -0.0704,  0.5602,  0.0072, -0.1228, -0.1776,\n",
      "         -0.3513, -0.2777, -0.1257, -0.1873,  0.0254, -0.0900, -0.3189, -0.2981]])\n",
      "auto_model.heads.11.skip.bias: tensor([-0.2079])\n",
      "auto_model.heads.12.mlp.0.weight: tensor([[ 0.0515,  0.0171,  0.0330,  ..., -0.1056, -0.0666,  0.1539],\n",
      "        [-0.3463, -0.0439,  0.1590,  ..., -0.1811,  0.0560,  0.0146],\n",
      "        [-0.4507, -0.0570, -0.0977,  ..., -0.1915, -0.1833,  0.1890],\n",
      "        ...,\n",
      "        [-0.1451,  0.0755,  0.1297,  ...,  0.0499,  0.0861,  0.0240],\n",
      "        [-0.2795, -0.1451,  0.1757,  ...,  0.0748, -0.1115,  0.1852],\n",
      "        [-0.1155,  0.1453,  0.0418,  ..., -0.1905, -0.1109, -0.1642]])\n",
      "auto_model.heads.12.mlp.0.bias: tensor([-0.2959, -0.1750, -0.2213, -0.2370, -0.1656, -0.1961,  0.0440, -0.1639,\n",
      "        -0.1756, -0.2181, -0.1763,  0.1167,  0.0556, -0.2011, -0.4128, -0.2040,\n",
      "         0.0745,  0.1120, -0.1864, -0.0210, -0.3113,  0.1460, -0.1795,  0.1173,\n",
      "         0.1926, -0.1657, -0.1706, -0.1975,  0.1576, -0.4411, -0.1551,  0.0774])\n",
      "auto_model.heads.12.mlp.3.weight: tensor([[ 0.0343,  0.0675,  0.0448, -0.0524, -0.1233,  0.0544, -0.0599, -0.0396,\n",
      "          0.0314,  0.0453,  0.0612, -0.0385, -0.1429,  0.0326,  0.1166,  0.0653,\n",
      "          0.1382, -0.0443,  0.0362,  0.0752,  0.0448, -0.0954, -0.0509, -0.1101,\n",
      "         -0.0608, -0.1855, -0.1368,  0.0194, -0.0283,  0.0653,  0.0342, -0.0585]])\n",
      "auto_model.heads.12.mlp.3.bias: tensor([-0.2382])\n",
      "auto_model.heads.12.skip.weight: tensor([[ 0.1099,  0.1510,  0.2136, -0.1254,  0.3275,  0.1885, -0.1152,  0.2298,\n",
      "         -0.1096,  0.2016,  0.0165, -0.2075, -0.0707, -0.2423,  0.1738,  0.2379,\n",
      "          0.4214,  0.0366, -0.0388, -0.0475,  0.5393,  0.2312, -0.2297, -0.0185,\n",
      "         -0.1182, -0.1420,  0.1014, -0.0541,  0.4129, -0.1745, -0.1908, -0.1126]])\n",
      "auto_model.heads.12.skip.bias: tensor([-0.0372])\n",
      "auto_model.heads.13.mlp.0.weight: tensor([[-0.3220, -0.1670, -0.4290,  ...,  0.0326, -0.1176, -0.0215],\n",
      "        [-0.2397, -0.1724, -0.3802,  ..., -0.0093, -0.1636, -0.0318],\n",
      "        [-0.2716, -0.1598, -0.3531,  ..., -0.0645,  0.0782, -0.2311],\n",
      "        ...,\n",
      "        [-0.3396, -0.2140, -0.5951,  ...,  0.1768, -0.2339, -0.0973],\n",
      "        [-0.1464, -0.4558, -0.0751,  ...,  0.0483,  0.0080,  0.1412],\n",
      "        [-0.3240, -0.0521, -0.6250,  ..., -0.0930, -0.1988,  0.0157]])\n",
      "auto_model.heads.13.mlp.0.bias: tensor([-0.3114, -0.4298,  0.2379, -0.6547, -0.1675,  0.1191, -0.2382, -0.3212,\n",
      "        -0.5118, -0.5138, -0.0566, -0.3964, -0.2960, -0.0364, -0.5269,  0.1673,\n",
      "         0.0908, -0.0049, -0.3539, -0.0141,  0.0014,  0.0626, -0.1724, -0.4028,\n",
      "        -0.4733,  0.0332, -0.3536, -0.2566,  0.1057, -0.4155, -0.2311, -0.2753])\n",
      "auto_model.heads.13.mlp.3.weight: tensor([[ 0.2890,  0.2578, -0.0319,  0.5596, -0.0299, -0.0493, -0.0597,  0.1262,\n",
      "          0.4366,  0.1991, -0.0658,  0.5999,  0.1206, -0.1276,  0.5298, -0.0554,\n",
      "         -0.1020, -0.0530,  0.1857, -0.0527, -0.1013, -0.0659, -0.1219,  0.1501,\n",
      "          0.2859, -0.0674,  0.1400,  0.1782,  0.0558,  0.4258, -0.1626,  0.1681]])\n",
      "auto_model.heads.13.mlp.3.bias: tensor([-0.0994])\n",
      "auto_model.heads.13.skip.weight: tensor([[-0.0421,  0.0095, -0.0512,  0.5061, -0.0274,  0.0090, -0.0301,  0.2698,\n",
      "         -0.1292,  0.4519,  0.0699,  0.1705, -0.1650,  0.0059, -0.0893, -0.0476,\n",
      "         -0.0892, -0.0924, -0.0095,  0.1935, -0.1510,  0.2708,  0.2997, -0.3447,\n",
      "          0.4110,  0.0881,  0.5930,  0.4784,  0.1254,  0.3071,  0.1319,  0.3455]])\n",
      "auto_model.heads.13.skip.bias: tensor([-0.1540])\n",
      "auto_model.heads.14.mlp.0.weight: tensor([[-0.1199,  0.0059, -0.0524,  ..., -0.1707, -0.0489,  0.4285],\n",
      "        [-0.0115, -0.0851, -0.2196,  ..., -0.0404, -0.0553,  0.4811],\n",
      "        [-0.0040, -0.3732, -0.2442,  ..., -0.0934, -0.2736, -0.5170],\n",
      "        ...,\n",
      "        [-0.0604,  0.0694, -0.4628,  ..., -0.0918, -0.0046,  0.3847],\n",
      "        [-0.0580,  0.0537, -0.1949,  ..., -0.1245,  0.0359, -0.4615],\n",
      "        [ 0.0453, -0.1099, -0.5994,  ..., -0.0684, -0.2035, -0.2904]])\n",
      "auto_model.heads.14.mlp.0.bias: tensor([-0.3075, -0.2329,  0.1498,  0.1021, -0.0292, -0.2745, -0.1364, -0.0342,\n",
      "         0.0815, -0.0834, -0.1391, -0.0527,  0.1349, -0.2243,  0.1662, -0.3819,\n",
      "        -0.3252, -0.0101, -0.3732, -0.0622, -0.2675,  0.1007, -0.1459, -0.2741,\n",
      "        -0.1055,  0.1223, -0.2177, -0.2049, -0.0202, -0.3678,  0.1106, -0.1117])\n",
      "auto_model.heads.14.mlp.3.weight: tensor([[ 0.1095,  0.0914, -0.0320,  0.0700,  0.0354,  0.0556,  0.1585, -0.0281,\n",
      "         -0.0169, -0.0503,  0.0756, -0.0172, -0.0472,  0.0710, -0.0227,  0.0760,\n",
      "          0.0992, -0.0343,  0.1307, -0.0833,  0.0923, -0.0413,  0.0679,  0.0892,\n",
      "          0.1012, -0.0287,  0.1010,  0.0909, -0.0273,  0.0887, -0.0344, -0.0662]])\n",
      "auto_model.heads.14.mlp.3.bias: tensor([-0.0255])\n",
      "auto_model.heads.14.skip.weight: tensor([[-0.0736, -0.0841, -0.1177,  0.4584, -0.0029,  0.0766, -0.1595,  0.2082,\n",
      "         -0.1234,  0.4151,  0.0621,  0.0423, -0.0761, -0.1631, -0.0791,  0.0149,\n",
      "         -0.0588, -0.1376, -0.0649,  0.2841, -0.1084,  0.1451,  0.1881, -0.3396,\n",
      "          0.5079,  0.0593,  0.6729,  0.5268,  0.1206,  0.3088,  0.1240,  0.3625]])\n",
      "auto_model.heads.14.skip.bias: tensor([0.0474])\n",
      "auto_model.heads.15.mlp.0.weight: tensor([[-0.0836,  0.0954, -0.2514,  ...,  0.0711,  0.3278,  0.2047],\n",
      "        [-0.0982,  0.0421,  0.2889,  ..., -0.1645, -0.3495, -0.3347],\n",
      "        [-0.1609, -0.1945,  0.0794,  ..., -0.0302, -0.2898, -0.4227],\n",
      "        ...,\n",
      "        [ 0.1773,  0.2014, -0.4794,  ..., -0.1003,  0.1511,  0.3205],\n",
      "        [-0.1036, -0.2130, -0.2951,  ..., -0.0025,  0.2667,  0.2202],\n",
      "        [ 0.1724,  0.1862, -0.3321,  ..., -0.0729,  0.3218,  0.3403]])\n",
      "auto_model.heads.15.mlp.0.bias: tensor([-0.2617, -0.0195,  0.0681,  0.0733, -0.1916,  0.0778, -0.2527, -0.2564,\n",
      "        -0.1339, -0.3436, -0.0789, -0.1127, -0.1332, -0.1338, -0.2033,  0.0414,\n",
      "        -0.1367, -0.2808, -0.0847, -0.0539,  0.0377, -0.2276,  0.0703, -0.1199,\n",
      "        -0.2323, -0.2783, -0.0155,  0.0200, -0.2923, -0.2514, -0.2483, -0.0360])\n",
      "auto_model.heads.15.mlp.3.weight: tensor([[ 0.0789, -0.0382, -0.0470, -0.0171,  0.0503, -0.0629,  0.1401,  0.0506,\n",
      "          0.1239,  0.0791,  0.1853,  0.0861,  0.0959,  0.0788,  0.0595, -0.0463,\n",
      "         -0.0289,  0.0694, -0.0474, -0.0391, -0.0420,  0.0495, -0.0576, -0.0293,\n",
      "          0.1243, -0.0515, -0.0678, -0.0531,  0.0826,  0.1469,  0.1319,  0.0869]])\n",
      "auto_model.heads.15.mlp.3.bias: tensor([-0.1515])\n",
      "auto_model.heads.15.skip.weight: tensor([[-0.0649, -0.0797, -0.0557,  0.4436,  0.0049,  0.0877, -0.1395,  0.1803,\n",
      "         -0.2126,  0.2791,  0.0325,  0.2318, -0.0551, -0.0947, -0.0982, -0.0258,\n",
      "         -0.0689, -0.0061,  0.0967,  0.3177, -0.1826,  0.1966,  0.2941, -0.3330,\n",
      "          0.4490,  0.0767,  0.6377,  0.4927,  0.1079,  0.3710,  0.2960,  0.3418]])\n",
      "auto_model.heads.15.skip.bias: tensor([0.0869])\n",
      "auto_model.heads.16.mlp.0.weight: tensor([[-0.1032,  0.0940,  0.0084,  ...,  0.2230, -0.0347,  0.0847],\n",
      "        [-0.1427, -0.0944, -0.1024,  ..., -0.0370,  0.1290, -0.0794],\n",
      "        [-0.2682,  0.0663,  0.5081,  ..., -0.1361, -0.0951,  0.1379],\n",
      "        ...,\n",
      "        [-0.0650, -0.1300, -0.1010,  ..., -0.2458,  0.0106, -0.0085],\n",
      "        [-0.1898, -0.0580, -0.1236,  ...,  0.1110, -0.0644, -0.1100],\n",
      "        [-0.2501, -0.1314, -0.1500,  ..., -0.0029,  0.0342,  0.0167]])\n",
      "auto_model.heads.16.mlp.0.bias: tensor([-0.0585, -0.2633,  0.1121,  0.1455,  0.0590, -0.1491, -0.0096,  0.0622,\n",
      "        -0.1882, -0.0901, -0.3429,  0.1888, -0.3153, -0.1920, -0.3080,  0.0233,\n",
      "        -0.1649, -0.1385, -0.4155, -0.3392, -0.3470, -0.2185, -0.1803, -0.0684,\n",
      "        -0.1465,  0.0897, -0.2502,  0.0176, -0.1173, -0.1620, -0.1118, -0.2436])\n",
      "auto_model.heads.16.mlp.3.weight: tensor([[-0.0200,  0.0415, -0.0385, -0.0535, -0.0520,  0.0116, -0.0371, -0.0224,\n",
      "         -0.0248,  0.0238,  0.0957, -0.0119,  0.0534,  0.0389,  0.0214, -0.0385,\n",
      "          0.0235, -0.0423,  0.0488,  0.0646,  0.0371, -0.0227,  0.0555, -0.0063,\n",
      "          0.0070,  0.0564,  0.0513, -0.0377, -0.0881,  0.0296,  0.0382,  0.0596]])\n",
      "auto_model.heads.16.mlp.3.bias: tensor([0.0482])\n",
      "auto_model.heads.16.skip.weight: tensor([[ 0.0126,  0.0402,  0.2502, -0.1780,  0.4856, -0.0404,  0.1614,  0.0893,\n",
      "         -0.0311,  0.0533,  0.0337, -0.1961,  0.5579, -0.3552,  0.0707,  0.2894,\n",
      "          0.6366, -0.0551,  0.2824, -0.0272,  0.1519, -0.0837, -0.2498, -0.0501,\n",
      "          0.3337, -0.0629, -0.0495,  0.0975,  0.5084,  0.3259, -0.1316, -0.0878]])\n",
      "auto_model.heads.16.skip.bias: tensor([-0.1661])\n",
      "correction_net.net.0.weight: tensor([[-1.1547e-01,  1.0162e-01, -5.6652e-02,  2.5670e-01, -7.9580e-02,\n",
      "         -3.7165e-02, -2.0281e-01,  1.9707e-01, -6.9473e-02,  1.9241e-01,\n",
      "         -2.0412e+00,  2.7316e-01,  1.3412e-01, -2.1216e-01,  2.4388e-02,\n",
      "         -2.3888e-01,  1.8442e-01, -7.3923e-02, -9.2291e-03,  2.2561e-02],\n",
      "        [-5.8492e-01, -1.8930e-01, -1.5812e-01, -2.8447e-01, -2.6076e-01,\n",
      "          3.0009e-01,  8.4850e-03, -6.2387e-02,  4.3141e-02,  2.0436e-01,\n",
      "         -3.3832e-02,  1.7080e-01, -5.3908e-02,  2.7188e-01, -3.4816e-03,\n",
      "         -1.9533e-01, -4.6418e-01, -2.5046e-01, -1.5231e-01, -2.3681e-01],\n",
      "        [-1.2150e-01, -7.7376e-03,  1.8909e-01, -1.2505e-01,  2.9607e-01,\n",
      "          2.0573e-02,  2.0785e-04,  5.0111e-01,  2.2117e-01,  4.6210e-03,\n",
      "          7.3385e-02, -9.5310e-02, -1.5635e-01,  2.7594e-01,  2.0503e-01,\n",
      "         -2.3412e-01,  2.2443e-01,  4.7090e-02, -1.1704e-01, -4.0392e-01],\n",
      "        [-2.7348e-01,  1.1132e-02, -2.2786e-02,  4.3720e-02, -1.6451e-01,\n",
      "         -5.4469e-02, -1.4959e-02, -5.2301e-02,  3.5103e-01, -3.4010e-01,\n",
      "          2.2249e-01,  3.3904e-01,  6.8803e-02, -1.2175e-01,  1.3871e-01,\n",
      "          2.6800e-02,  7.8109e-02, -3.7164e-02,  5.3200e-02, -3.9292e-01],\n",
      "        [ 1.7357e-01, -9.7600e-02,  6.4267e-02, -2.3469e-01, -5.6746e-02,\n",
      "          1.0054e-01, -4.4550e-02,  5.4010e-02,  4.6782e-01, -6.4574e-01,\n",
      "         -5.4747e-01, -9.2256e-02, -1.8218e-01,  2.0851e-01, -1.3155e-02,\n",
      "         -5.8429e-02,  1.8808e-01, -3.9137e-02, -2.3769e-01, -4.9933e-02],\n",
      "        [ 1.0048e-02,  2.2991e-01, -9.8348e-02,  3.2091e-01, -7.9332e-02,\n",
      "          2.5303e-01, -4.5809e-02, -3.8837e-01,  2.4238e-01,  8.1987e-02,\n",
      "         -9.1121e-02, -6.9610e-02,  7.4061e-02, -2.5496e-02, -5.8186e-02,\n",
      "         -1.2149e-01, -5.5525e-02,  2.6590e-01,  1.0832e-01,  1.4653e-01],\n",
      "        [-2.5277e-01,  5.5199e-02,  3.2253e-01,  1.3835e-01,  1.8555e-01,\n",
      "          2.7246e-01, -2.9492e-01,  8.9681e-02, -5.6131e-02, -3.2929e-01,\n",
      "          9.8049e-02, -1.2366e-01,  4.5268e-02,  3.8663e-02,  4.6882e-02,\n",
      "          2.3839e-01,  2.6615e-01, -1.2525e-01,  4.3143e-02,  3.4967e-01],\n",
      "        [-2.0347e-01,  1.0818e-01, -8.5289e-02, -2.4435e-01,  2.7076e-01,\n",
      "         -8.0663e-02,  2.1279e-01, -1.0569e-01, -9.9425e-02, -4.5383e-03,\n",
      "          1.2096e-01, -2.3674e-01,  1.2765e-01,  3.4141e-02, -2.5852e-01,\n",
      "         -5.3956e-02, -2.0972e-01, -2.5409e-01,  4.4815e-02,  1.1166e-01],\n",
      "        [-2.3474e-01, -1.3489e-02,  1.5161e-01,  3.1697e-02,  2.6428e-02,\n",
      "          4.8956e-01, -3.5657e-01,  6.4379e-02, -6.8484e-02, -7.0837e-02,\n",
      "         -2.1206e-01, -1.0648e-01, -6.5578e-02,  1.8002e-01,  2.0510e-01,\n",
      "          2.6915e-01, -3.2228e-01, -1.4959e-02, -5.1038e-02,  6.4549e-03],\n",
      "        [-2.4276e-01,  1.9759e-02, -3.4093e-02,  6.9673e-02, -1.2119e-01,\n",
      "          4.6049e-01,  8.5413e-02, -1.3220e-01, -8.9683e-02,  7.1922e-02,\n",
      "         -5.6174e-02, -1.1836e-01,  2.2191e-02, -8.5871e-02, -2.9587e-01,\n",
      "         -1.7585e-01, -2.2838e-03, -2.9003e-01,  2.1842e-01,  6.7017e-02],\n",
      "        [-1.1039e-01,  1.9199e-01, -2.0772e-01,  1.3226e-01, -2.4978e-01,\n",
      "          1.6406e-01,  5.3738e-02,  8.4225e-02,  8.0187e-03, -2.1020e-01,\n",
      "          8.9918e-02,  3.1049e-02, -4.7055e-04,  1.7725e-02, -1.5279e-01,\n",
      "         -2.0409e-01,  1.3151e-01,  5.7674e-02, -7.4835e-03,  2.3730e-01],\n",
      "        [ 3.6644e-02, -9.6461e-02,  7.7067e-02, -8.4446e-02,  3.2073e-01,\n",
      "         -6.2406e-02,  6.6885e-02,  2.6056e-01,  3.3344e-02,  4.1745e-01,\n",
      "         -4.9319e-02, -1.4098e-01, -1.5218e-01,  1.4525e-02,  7.0560e-02,\n",
      "         -3.1011e-02, -2.4474e-01,  9.9130e-02,  1.3838e-01, -6.9997e-01],\n",
      "        [-3.1555e-01, -9.6330e-02,  6.2049e-02,  1.1070e-01,  1.2880e-01,\n",
      "          7.6516e-02, -2.9940e-02, -1.3152e-01, -4.5165e-02, -5.1949e-02,\n",
      "          5.4375e-03, -1.7058e-01,  1.0493e-01, -6.3702e-02,  3.6691e-02,\n",
      "         -9.2331e-02, -9.4714e-03, -7.3917e-02, -8.4948e-02,  3.4280e-01],\n",
      "        [ 1.1789e-01,  8.6174e-02, -2.2568e-02, -6.5047e-02,  3.3081e-01,\n",
      "          1.3642e-02, -2.4422e-03, -9.6599e-03, -2.1122e-02,  8.6681e-02,\n",
      "          9.8149e-03,  4.5899e-02, -1.7828e-01,  4.9316e-02,  4.0158e-02,\n",
      "         -2.1773e-01, -9.6404e-03,  5.9707e-01, -3.6317e-01, -2.1817e-01],\n",
      "        [ 8.0367e-02, -1.3466e-02,  8.8207e-02, -2.5817e-02, -1.1084e-01,\n",
      "         -3.4638e-02,  1.5046e-01,  4.2284e-02,  4.7470e-02, -6.5722e-02,\n",
      "         -4.9302e-02, -8.6272e-02, -5.8276e-02,  7.3245e-02,  1.4899e-02,\n",
      "         -6.4026e-02, -4.7023e-01,  1.9800e-01,  4.3745e-01, -6.3596e-02],\n",
      "        [-5.3059e-01,  1.2771e-01, -2.0116e-01,  2.0123e-01,  8.4721e-02,\n",
      "         -2.4146e-02,  1.3480e-01, -4.6311e-01,  2.1735e-01,  2.5674e-01,\n",
      "         -3.9523e-01, -1.9128e-03,  9.3066e-03, -2.9621e-02, -2.1805e-01,\n",
      "          2.2365e-01, -1.4683e-01, -4.0968e-01,  5.5895e-02, -9.2546e-02],\n",
      "        [ 2.1676e-01,  6.3595e-02, -1.0764e-01, -1.4206e-01,  2.3333e-01,\n",
      "          1.2974e-01, -5.1213e-02, -1.7177e-01,  2.2130e-01,  2.4604e-01,\n",
      "         -1.4697e-01, -3.5611e-01, -1.0046e-01,  1.9706e-01, -5.8462e-02,\n",
      "         -3.1566e-01, -6.8289e-02,  9.4555e-02,  3.1291e-01,  1.3753e-01],\n",
      "        [ 1.1962e-01,  1.1438e-01, -3.8464e-02, -7.0126e-02,  4.4766e-02,\n",
      "         -6.6931e-02,  4.4184e-02, -1.8225e-01, -2.5148e-02,  3.8135e-02,\n",
      "          9.5591e-02,  6.7318e-02, -4.1868e-02,  1.4418e-02, -1.7556e-02,\n",
      "         -1.9818e-01,  4.5843e-01, -1.6540e-01, -3.0193e-01, -3.9566e-01],\n",
      "        [-1.2862e-01, -1.3992e-01,  2.6550e-02,  2.2324e-01, -4.4779e-02,\n",
      "         -2.2993e-01,  2.5820e-01,  1.1149e-01,  6.1911e-02, -5.4221e-03,\n",
      "         -1.1115e-02, -2.4610e-01, -2.1037e-02, -1.3233e-01,  1.7134e-01,\n",
      "         -2.7506e-01, -4.6071e-01,  3.6934e-01,  2.4112e-01,  6.0074e-02],\n",
      "        [ 3.0151e-01, -9.5517e-03,  1.1162e-01, -1.8281e-01,  2.0517e-01,\n",
      "          2.2201e-01,  1.4868e-01, -6.2506e-01, -1.8729e-01,  1.7558e-01,\n",
      "         -5.2426e-02, -6.2711e-02, -3.6395e-01, -1.3646e-01,  1.1811e-01,\n",
      "          3.1937e-02,  1.6277e-01,  4.9933e-02, -4.7006e-02,  4.1561e-01],\n",
      "        [ 3.3910e-02, -1.1701e-01,  1.1632e-01,  2.0039e-01,  3.0080e-01,\n",
      "         -1.9544e-01,  1.0497e-01, -3.2767e-01, -1.8782e-01, -5.3692e-02,\n",
      "          1.6819e-01,  2.2514e-01, -3.9479e-02, -1.4957e-01,  9.2608e-02,\n",
      "         -2.0557e-01, -1.3112e-01,  6.4394e-02,  3.5507e-01, -5.2310e-01],\n",
      "        [ 1.0972e-01,  6.4471e-02,  2.7909e-01, -9.3427e-02,  1.3659e-01,\n",
      "          1.6884e-01,  1.9496e-01, -2.6084e-01,  7.1851e-02,  1.7323e-01,\n",
      "         -2.9632e-03, -1.8016e-01, -7.8723e-02,  2.9447e-01,  1.2275e-01,\n",
      "         -3.0421e-02, -1.0683e-02,  1.5933e-01, -9.1346e-02, -6.8353e-02],\n",
      "        [ 1.6225e-01,  1.4931e-01, -5.5982e-01, -9.6757e-02, -3.3871e-02,\n",
      "         -3.0052e-01, -3.2072e-01, -1.8965e-01, -1.9848e-01,  1.5032e-01,\n",
      "         -4.6604e-02,  1.9558e-01,  2.9902e-02, -1.2820e-02,  1.4386e-01,\n",
      "          7.7666e-02, -1.2908e-01, -2.5806e-02, -5.2908e-02,  1.0549e-01],\n",
      "        [ 2.7465e-01,  1.1035e-01,  1.8862e-02,  1.7004e-01,  1.0215e-01,\n",
      "          8.2539e-02, -2.1568e-01,  7.3291e-02,  2.0765e-01,  1.4669e-01,\n",
      "         -7.9312e-02, -2.7043e-01, -2.1876e-01,  1.5276e-01, -8.8489e-02,\n",
      "         -3.5655e-01, -2.5461e-02, -1.0391e-02,  1.7111e-01,  2.9938e-01],\n",
      "        [-4.5128e-02, -6.5631e-04, -9.9571e-03, -8.2692e-02, -4.8850e-02,\n",
      "          5.0576e-03, -1.4415e-01, -7.4143e-01,  3.3261e-01,  3.5611e-01,\n",
      "         -1.2613e-01,  2.6270e-01,  3.5362e-02, -3.4643e-02, -1.0171e-01,\n",
      "          2.4857e-01, -3.2945e-01,  1.3477e-01,  2.6211e-01, -3.6483e-02],\n",
      "        [-1.2314e-01,  1.0706e-01, -1.6084e-01, -5.4041e-02, -1.3065e-01,\n",
      "         -1.0477e-01,  7.0382e-02, -2.9994e-02, -1.9621e-01,  2.7792e-02,\n",
      "          1.5543e-01,  3.9227e-01,  2.2315e-01, -1.9720e-01, -4.7081e-02,\n",
      "         -2.9703e-01, -3.5356e-01, -2.8859e-01, -3.9601e-01,  2.9154e-01],\n",
      "        [ 9.5017e-02,  1.2226e-02, -6.4138e-02, -2.0368e-01, -4.9752e-02,\n",
      "          1.0175e-01,  3.0436e-01,  3.6399e-02,  1.1557e-02,  2.1045e-01,\n",
      "          7.4787e-03, -2.6440e-01, -8.9471e-03,  4.6516e-02, -8.7835e-02,\n",
      "          6.5621e-03,  1.7628e-01,  1.9115e-01, -4.2255e-01, -3.9906e-01],\n",
      "        [ 8.1042e-02,  4.7283e-02, -5.9180e-02, -8.6327e-02, -6.1237e-02,\n",
      "         -3.3959e-02, -2.9648e-01,  2.2814e-01,  1.1726e-01, -4.0324e-01,\n",
      "          1.9316e-01,  3.0424e-01, -2.1609e-01,  1.6588e-01, -1.2867e-01,\n",
      "         -1.9999e-01,  1.2639e-03,  7.9362e-02, -1.5665e-01,  2.8040e-01],\n",
      "        [ 1.1101e-01,  4.7352e-02, -2.4081e-02, -4.2982e-02,  1.1930e-01,\n",
      "         -1.8686e-01, -7.3180e-02, -3.0816e-01, -1.2215e-01,  2.9411e-01,\n",
      "         -1.0287e-01,  6.0006e-02,  5.2874e-02, -3.4300e-02, -1.3799e-01,\n",
      "          1.4686e-01,  5.0107e-01, -3.6280e-01, -5.2465e-02,  8.4843e-02],\n",
      "        [-3.4106e-01,  2.4240e-02, -1.9998e-01, -1.2591e-01, -4.2050e-01,\n",
      "          2.0894e-02, -2.5488e-02, -5.9167e-02,  3.3634e-02,  1.1884e-01,\n",
      "          7.9846e-02,  2.8006e-01,  1.2405e-01, -1.3620e-01, -1.0473e-01,\n",
      "          5.8929e-02, -2.1768e-01, -1.8375e-01, -4.0829e-01, -5.6471e-02],\n",
      "        [ 1.6479e-01,  1.6379e-01, -3.5358e-02,  1.7045e-01,  1.5565e-01,\n",
      "          1.5699e-01,  1.6250e-01, -2.9799e-01, -1.5439e-01,  2.3665e-01,\n",
      "         -6.1302e-02, -3.7392e-01,  1.8776e-01, -5.7273e-02,  2.3414e-01,\n",
      "         -1.6312e-01,  2.3087e-02,  1.0922e-01,  1.6860e-01,  1.6042e-01],\n",
      "        [-3.2032e-01,  9.1207e-02, -8.8678e-02,  4.1728e-02, -4.4143e-03,\n",
      "         -8.3022e-02,  1.8945e-01, -7.5101e-02, -1.1239e-01,  1.2294e-01,\n",
      "          2.8416e-02,  2.4181e-01, -1.4253e-01,  1.4270e-01,  5.6240e-02,\n",
      "         -2.4914e-01, -1.2331e-01, -1.8726e-01, -3.1402e-01, -8.0224e-02]])\n",
      "correction_net.net.0.bias: tensor([-0.0438, -0.5863, -0.1709, -0.2044, -0.0348, -0.7429, -0.3462, -0.6067,\n",
      "        -0.4585, -0.4518, -0.7725, -0.2768, -0.4916, -0.2420, -0.3486, -0.2716,\n",
      "        -0.4308, -0.3595, -0.6048, -0.3142, -0.7819, -0.7856, -0.5375, -0.5788,\n",
      "        -0.1030, -0.2094, -0.1420,  0.0348, -0.3897, -0.3472, -0.7675, -0.4699])\n",
      "correction_net.net.2.weight: tensor([[ 9.2085e-04,  3.5153e-02,  5.3460e-03,  3.0775e-02,  9.1581e-02,\n",
      "         -2.1289e-02, -7.2148e-02, -6.3693e-02,  5.1542e-02,  1.3376e-02,\n",
      "         -3.1478e-02, -6.8709e-03, -3.4541e-02,  2.6077e-03,  5.2831e-02,\n",
      "         -1.0967e-01,  2.1826e-02, -6.6413e-02, -5.5894e-02,  1.0306e-01,\n",
      "         -1.2955e-02, -2.2747e-02, -4.5315e-02, -2.8472e-02,  3.8806e-03,\n",
      "          3.4340e-02,  2.8343e-02,  1.0211e-01,  3.4641e-03, -6.8114e-02,\n",
      "         -1.7249e-02, -3.5672e-03],\n",
      "        [ 3.7198e-03, -5.8495e-03,  5.2377e-04, -9.6238e-02,  4.5086e-02,\n",
      "         -2.2227e-02, -1.4104e-02, -2.4916e-02, -3.2611e-02, -2.8928e-02,\n",
      "          9.1104e-02,  1.1517e-02, -1.7091e-02, -3.2020e-03, -1.0304e-02,\n",
      "         -8.1958e-02, -3.9235e-02, -2.3455e-02,  8.2210e-02, -5.3142e-02,\n",
      "         -3.4684e-03, -5.3300e-02,  1.3853e-02,  5.3519e-02, -1.6040e-02,\n",
      "          5.0338e-02, -9.9459e-02,  9.5146e-02,  1.0719e-01, -1.2368e-01,\n",
      "          4.7463e-02, -1.1493e-01],\n",
      "        [-2.0902e-02, -7.3114e-02,  7.6789e-03, -1.6521e-01, -7.8662e-02,\n",
      "         -5.4630e-02, -1.1416e-01,  3.0236e-02, -1.3683e-01, -2.7873e-01,\n",
      "         -6.2244e-02, -1.2667e-02,  5.0936e-02,  1.1832e-02, -1.2243e-01,\n",
      "          5.2024e-02, -3.4274e-02,  1.5754e-01,  1.9822e-01, -2.0662e-01,\n",
      "          6.8873e-02, -8.5798e-03,  2.3498e-01,  6.0339e-02,  1.8065e-03,\n",
      "          4.8522e-02, -2.0433e-01,  2.7391e-02,  1.8854e-01,  2.6662e-02,\n",
      "         -4.9308e-02, -2.1319e-04],\n",
      "        [ 1.7900e-02, -1.6672e-02,  6.8388e-03,  5.0761e-02,  1.8280e-02,\n",
      "         -5.5731e-03,  6.0294e-02, -2.9254e-03,  2.8602e-02,  6.4891e-03,\n",
      "         -1.2303e-02,  2.0458e-02, -2.0627e-02, -4.0020e-02, -1.2504e-01,\n",
      "          7.1654e-03,  7.9348e-02,  2.9964e-02, -1.0764e-01, -7.1521e-02,\n",
      "         -6.4753e-02, -2.3209e-02,  2.8021e-01,  7.5391e-02,  1.8761e-01,\n",
      "         -3.0842e-03,  1.0971e-01,  6.1091e-02,  5.3943e-04, -1.2269e-01,\n",
      "          2.6967e-02, -1.3976e-01],\n",
      "        [ 3.9932e-03,  2.5288e-02, -2.0196e-01, -5.0286e-03, -4.6141e-02,\n",
      "          6.8127e-03,  2.0360e-02, -1.4618e-02, -1.2904e-02, -5.4227e-04,\n",
      "         -6.1670e-03, -1.5974e-01,  4.0037e-02,  1.0071e-02, -1.2322e-01,\n",
      "          1.2740e-01, -9.7559e-02,  1.9566e-01, -1.5641e-01,  2.7975e-01,\n",
      "          8.0780e-02,  1.7189e-02,  1.4190e-01, -1.0372e-01,  4.8921e-01,\n",
      "          1.6339e-02, -1.4850e-02, -7.9631e-02, -5.6337e-02,  5.8973e-02,\n",
      "         -1.0558e-01,  6.8525e-02],\n",
      "        [-2.1660e-02,  7.7088e-02, -5.8761e-02, -1.3792e-01, -1.4205e-01,\n",
      "         -5.7745e-02, -1.7594e-03, -1.4821e-02, -2.5103e-02,  4.9407e-02,\n",
      "          4.9039e-02,  6.1227e-02, -1.3197e-02, -5.7090e-02, -2.4174e-02,\n",
      "          8.7910e-02, -1.4892e-02,  1.2113e-01,  1.2378e-01,  1.0092e-01,\n",
      "          6.1940e-02,  1.6761e-02,  1.9274e-01, -5.7552e-03, -4.7016e-02,\n",
      "          1.0244e-01, -1.4777e-02, -5.4619e-02,  3.4316e-02,  1.2664e-05,\n",
      "         -1.3176e-02, -3.6202e-02],\n",
      "        [ 1.3329e-01,  4.4231e-02, -4.4068e-02,  2.4892e-01,  3.2820e-01,\n",
      "          2.4384e-02,  1.1875e-01,  1.1678e-01,  1.2204e-01, -2.2665e-02,\n",
      "          2.3481e-01, -3.7196e-01,  1.4754e-01, -4.0095e-02,  1.7714e-01,\n",
      "         -2.6357e-02,  6.7260e-02, -4.2300e-02,  8.6956e-02, -4.8864e-02,\n",
      "         -1.9966e-01, -2.2699e-01,  9.1987e-03,  6.9642e-02, -1.8789e-01,\n",
      "          7.2447e-02, -8.4831e-02,  2.6928e-01, -1.5238e-01,  1.3780e-01,\n",
      "         -1.8981e-01,  2.4101e-02],\n",
      "        [ 4.1531e-01,  1.5828e-02, -4.8156e-02, -2.7714e-01,  2.2110e-01,\n",
      "          1.6054e-01, -1.0689e-01,  5.7068e-02,  1.5825e-02,  3.1841e-02,\n",
      "         -2.8129e-02,  1.2240e-01, -8.5424e-03, -1.9354e-03,  1.6300e-01,\n",
      "         -9.6011e-02,  2.7890e-01, -1.7744e-01,  1.1331e-01,  6.8529e-02,\n",
      "         -6.0592e-01,  7.9415e-02,  1.3580e-02,  7.9087e-02,  2.4418e-01,\n",
      "         -2.4816e-01,  1.1380e-02, -1.6282e-01,  1.3253e-01, -2.1550e-01,\n",
      "          1.4061e-01, -1.3958e-01],\n",
      "        [ 9.2868e-03, -1.0690e-01,  1.1814e-02, -5.1894e-02, -4.6004e-02,\n",
      "         -2.9200e-03,  4.7429e-02,  1.0531e-01, -1.4324e-02,  2.8576e-02,\n",
      "         -5.8015e-02,  8.2332e-03,  3.2591e-02, -8.6333e-02, -4.1747e-02,\n",
      "          1.8148e-02,  1.1915e-01,  1.0789e-01,  2.2849e-02,  3.4046e-02,\n",
      "         -2.3607e-02,  5.5089e-02,  1.2979e-01,  6.7540e-02, -1.0410e-01,\n",
      "         -1.0278e-01,  1.2585e-01, -3.0423e-02, -6.4497e-02, -1.0907e-01,\n",
      "          8.4940e-02, -1.1148e-01],\n",
      "        [-1.0456e-02, -3.5943e-02,  1.6590e-02,  1.5419e-02,  6.1067e-02,\n",
      "          1.5484e-02, -6.9301e-02, -3.3400e-02,  2.0498e-02,  3.2294e-02,\n",
      "         -1.8315e-02,  1.6630e-02, -6.7776e-02, -4.4563e-02,  1.3386e-02,\n",
      "         -6.9818e-02, -2.4346e-03,  7.0889e-03, -1.1177e-02,  1.2118e-01,\n",
      "         -1.3695e-02,  9.0460e-02, -3.6804e-02,  3.3147e-02, -7.0884e-03,\n",
      "         -6.7771e-02,  3.1388e-02,  6.4402e-02, -2.3890e-02, -5.2054e-02,\n",
      "         -4.6754e-02,  1.0601e-02],\n",
      "        [ 7.4654e-03, -2.6686e-02, -1.8498e-02,  1.0148e-02, -3.6402e-02,\n",
      "         -3.7393e-02,  3.6490e-02, -4.0224e-02, -5.6589e-02,  6.0679e-02,\n",
      "          1.9316e-02,  3.3535e-02, -2.7268e-02, -7.9866e-03, -1.3143e-02,\n",
      "         -3.2818e-02, -3.1262e-02, -3.0365e-02,  4.6727e-02,  1.2262e-01,\n",
      "          2.8215e-02, -9.9750e-02,  1.7873e-02, -4.1172e-02, -3.5281e-02,\n",
      "          2.0267e-02,  3.1699e-02, -4.7831e-02, -8.3148e-03,  8.2296e-03,\n",
      "         -5.5480e-02, -4.5387e-02],\n",
      "        [ 3.0333e-03,  1.6379e-03, -5.1100e-02, -1.0897e-01,  1.4938e-02,\n",
      "         -1.1462e-02, -2.1816e-02, -1.3155e-02,  9.9665e-03,  5.6302e-02,\n",
      "         -1.6841e-02, -3.8912e-02,  2.1111e-03, -2.0841e-02, -7.3026e-02,\n",
      "         -2.0588e-02,  1.9593e-02,  3.5907e-02,  3.5172e-02, -2.9682e-02,\n",
      "          4.0214e-02, -7.5154e-02, -1.9223e-02,  1.8070e-02, -1.4190e-02,\n",
      "          7.6540e-02, -1.1687e-02,  7.4706e-02,  1.9968e-02, -1.1104e-01,\n",
      "         -6.1935e-02, -6.3435e-02],\n",
      "        [ 1.1340e-02,  5.1848e-02,  2.0241e-02, -6.4149e-02, -1.9040e-02,\n",
      "         -1.6170e-02, -8.2684e-02, -4.3864e-02, -5.1300e-02,  8.9127e-02,\n",
      "         -2.6196e-02, -2.6387e-02, -4.0897e-02, -5.8992e-02, -2.7828e-03,\n",
      "         -1.4047e-03,  6.2966e-02,  1.0382e-01,  8.4033e-02, -1.4432e-02,\n",
      "          1.6411e-02, -5.2120e-02,  1.3623e-01, -1.1524e-02, -8.0670e-02,\n",
      "          1.7657e-01,  5.8738e-03,  5.9681e-02, -1.6001e-02, -7.0138e-02,\n",
      "         -3.2650e-02,  6.1617e-03],\n",
      "        [-8.1889e-03,  3.3767e-01, -6.7045e-02,  3.0323e-02, -5.5007e-02,\n",
      "         -7.0230e-02, -1.3805e-01,  1.1371e-01,  1.1032e-01, -5.3644e-03,\n",
      "         -1.7301e-01,  2.0508e-02, -1.1455e-02,  1.2447e-01,  4.6689e-01,\n",
      "          1.0175e-01,  2.3461e-02, -3.8593e-01,  3.9565e-01, -1.2598e-01,\n",
      "          3.6982e-01, -3.8790e-02,  3.6107e-02, -1.2535e-01,  1.6717e-01,\n",
      "          2.6208e-01, -1.8349e-01,  7.0803e-03, -3.0463e-01,  1.0148e-01,\n",
      "         -2.3566e-01,  1.5994e-01],\n",
      "        [-4.2546e-04,  2.9772e-01, -2.3785e-02,  1.2691e-01, -3.5085e-02,\n",
      "         -2.6228e-01,  9.9572e-03,  7.7277e-02,  2.6814e-02,  1.4035e-01,\n",
      "         -4.1858e-03,  1.4117e-02,  3.4560e-02, -7.8777e-01,  2.6130e-01,\n",
      "          1.7469e-01,  2.3333e-02,  5.4525e-02, -8.3328e-03, -1.2663e-01,\n",
      "          8.3808e-02, -4.4989e-02, -1.9812e-02, -1.0044e-01, -6.7132e-03,\n",
      "          2.1556e-01, -1.2348e-01, -2.2986e-02,  3.1321e-01,  7.9818e-02,\n",
      "         -2.0956e-01,  9.2898e-02],\n",
      "        [-1.0545e-02,  3.1093e-01,  5.9775e-02, -4.0297e-03,  4.9590e-02,\n",
      "          1.3815e-01, -7.2452e-02, -4.0301e-01,  1.6886e-02, -1.0869e-01,\n",
      "          1.4314e-01, -4.1435e-02, -7.7583e-02,  4.0732e-01, -6.2079e-02,\n",
      "          1.4987e-01, -1.5538e-01,  1.3319e-01,  6.4160e-03,  1.2833e-02,\n",
      "         -7.7594e-02, -4.3642e-02, -4.5823e-03, -1.0117e-01, -1.0769e-01,\n",
      "          2.4559e-01,  2.9834e-01,  3.1510e-02, -1.7184e-01,  1.9177e-01,\n",
      "          1.9657e-01,  1.0969e-01],\n",
      "        [ 1.0721e-02,  7.7997e-02,  2.5961e-01, -5.0522e-03, -6.5684e-02,\n",
      "          1.0429e-03, -9.3077e-02, -1.1499e-01,  3.8720e-02, -2.1440e-02,\n",
      "          2.4763e-02,  1.5638e-01, -1.8726e-01, -6.3097e-02, -4.0821e-02,\n",
      "          1.3719e-01,  1.7479e-02,  1.4899e-01, -2.2725e-02, -1.4533e-01,\n",
      "          9.0392e-02, -8.8385e-02,  1.1837e-01, -6.0703e-02,  1.2462e-01,\n",
      "         -1.1294e-02,  7.7858e-02, -1.1968e-01, -8.3089e-02,  6.1278e-02,\n",
      "         -1.1814e-01,  2.0454e-02]])\n",
      "correction_net.net.2.bias: tensor([-0.0245, -0.0120,  0.0217, -0.0140,  0.0017,  0.0385, -0.1089, -0.0887,\n",
      "         0.0128, -0.0124,  0.0101, -0.0100, -0.0072, -0.0081, -0.0102, -0.0342,\n",
      "         0.0121])\n",
      "Total number of parameters: 25843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### counting the nb of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def print_model_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.data}\")\n",
    "    print(f\"Total number of parameters: {count_parameters(model)}\")\n",
    "    \n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test Loss:  0.005208857823163271\n",
    "# RMSE:  0.07217241732935978\n",
    "\n",
    "# Test Loss:  0.004562491551041603\n",
    "# RMSE:  0.06754621788850655\n",
    "\n",
    "# Test Loss: 0.0046\n",
    "# RMSE: 0.0675\n",
    "# Test MAE: 0.0254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_space = np.arange(0, num_epochs)\n",
    "\n",
    "teacher_forcing_ratio_linear = initial_teacher_forcing_ratio - \\\n",
    "            (initial_teacher_forcing_ratio - final_teacher_forcing_ratio) * epochs_space / num_epochs\n",
    "\n",
    "teacher_forcing_ratio_cosine = final_teacher_forcing_ratio + 0.5 * (initial_teacher_forcing_ratio - final_teacher_forcing_ratio) * \\\n",
    "                              (1 + np.cos(np.pi * epochs_space / num_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30914fc20>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs_space, teacher_forcing_ratio_linear)\n",
    "plt.plot(epochs_space, teacher_forcing_ratio_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
